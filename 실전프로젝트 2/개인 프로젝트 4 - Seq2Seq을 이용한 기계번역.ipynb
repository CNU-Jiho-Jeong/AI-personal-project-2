{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2pgM0w9Xa1"
      },
      "source": [
        "#### **Sequence to Sequence Learning with Neural Networks (NIPS 2014)** 실습\n",
        "* 본 코드는 기본적으로 **Seq2Seq** 논문의 내용을 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델은 Seq2Seq가 아닌 **Transformer 기반의 모델**입니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UOI3Xm4U4M"
      },
      "source": [
        "#### **데이터 전처리(Preprocessing)**\n",
        "\n",
        "* **spaCy 라이브러리**: 문장의 토큰화(tokenization), 태깅(tagging) 등의 전처리 기능을 위한 라이브러리\n",
        "  * 영어(Engilsh)와 독일어(Deutsch) 전처리 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x8SEN31g34aX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s7u-Xt2c4WV8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
        "spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰화(tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vnjcXO84aRa",
        "outputId": "60bb9550-d961-49ff-8f87-02524874eb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ],
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hskqq3f4-YT"
      },
      "source": [
        "* 영어(English) 및 독일어(Deutsch) **토큰화 함수** 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1EH0gEEb4iTj"
      },
      "outputs": [],
      "source": [
        "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수\n",
        "def tokenize_de(text):\n",
        "    return [token.text.lower() for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어(English) 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "    return [token.text.lower() for token in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "fBQSvMyu94Mc",
        "outputId": "de7f24bd-2569-4da0-a5e6-1ba5e7f6005f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.6.0 torchdata-0.4.1 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torchdata"
      ],
      "metadata": {
        "id": "H3U8hfqK94KE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-fAQN1O6_a9"
      },
      "source": [
        "* **필드(field)** 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
        "* 번역 목표\n",
        "    * 소스(SRC): 독일어\n",
        "    * 목표(TRG): 영어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3MN7tLQ8wzG"
      },
      "source": [
        "* 대표적인 영어-독어 번역 데이터셋인 **Multi30k**를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PaEW2EpY72jt"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "train_dataset= Multi30k(root='.data', split=('train'), language_pair=(\"de\", \"en\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en = []\n",
        "de = []\n",
        "for label, line in train_dataset:\n",
        "    de+=[tokenize_de(label)]\n",
        "    en+=[tokenize_en(line)]"
      ],
      "metadata": {
        "id": "YPyZ04wuAqNI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w31HzGx04qmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3a5c8e41-400a-4ad0-8446-e913693ec510"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-17d2adcb18c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTRG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torchdata\n",
        "from torchtext.dataset import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OgN2KkyJ6kf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "8b3c75aa-1bb1-4ff3-9339-084d6c5ab429"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a74dc9945a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/datapipe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{0}' object has no attribute '{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ShardingFilterIterDataPipe' object has no attribute 'examples"
          ]
        }
      ],
      "source": [
        "print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n",
        "print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n",
        "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6clmgtf7ENF"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 중 하나를 선택해 출력\n",
        "print(vars(train_dataset.examples[30])['src'])\n",
        "print(vars(train_dataset.examples[30])['trg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvPZWqQk9Ea1"
      },
      "source": [
        "* **필드(field)** 객체의 **build_vocab** 메서드를 이용해 영어와 독어의 단어 사전을 생성합니다.\n",
        "  * **최소 2번 이상** 등장한 단어만을 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sos_token='<sos>'\n",
        "eos_token='<eos>'"
      ],
      "metadata": {
        "id": "NeXsC70p1EPD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRG=torchtext.vocab.build_vocab_from_iterator(en,min_freq=2,specials=['<unk>','<pad>',sos_token,eos_token])\n",
        "SRC=torchtext.vocab.build_vocab_from_iterator(de,min_freq=2,specials=['<unk>','<pad>',sos_token,eos_token])\n",
        "TRG.set_default_index(0)\n",
        "SRC.set_default_index(0)"
      ],
      "metadata": {
        "id": "skqJ0JMb1HP6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(TRG.vocab))\n",
        "print(len(SRC.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdI3BlSQ1MCK",
        "outputId": "ca118504-1cb0-4495-bd87-2bbd38116908"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5893\n",
            "7853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WR7DqpLU8hiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770fc258-9255-46b8-ae20-deeddf775ae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'‘': 7852,\n",
              " 'üppigen': 7851,\n",
              " 'überwacht': 7849,\n",
              " 'übersäte': 7848,\n",
              " 'überschlag': 7845,\n",
              " 'überreste': 7844,\n",
              " 'überlegt': 7843,\n",
              " 'überhang': 7841,\n",
              " 'überführung': 7839,\n",
              " 'überfluteten': 7838,\n",
              " 'überfliegt': 7837,\n",
              " 'öffentliches': 7833,\n",
              " 'äste': 7832,\n",
              " 'äpfel': 7830,\n",
              " 'ähnlichen': 7828,\n",
              " 'zügel': 7827,\n",
              " 'zwirbelt': 7825,\n",
              " 'zweispurigen': 7822,\n",
              " 'zweirad': 7821,\n",
              " 'zwanzig': 7819,\n",
              " 'zustand': 7818,\n",
              " 'zuschauerraum': 7817,\n",
              " 'zusammensein': 7816,\n",
              " 'zusammenpassenden': 7815,\n",
              " 'zusammengebaut': 7813,\n",
              " 'zuneigung': 7811,\n",
              " 'zuhörern': 7808,\n",
              " 'zuhält': 7807,\n",
              " 'zugfahrt': 7804,\n",
              " 'zugedeckt': 7802,\n",
              " 'zufährt': 7801,\n",
              " 'zufahrt': 7800,\n",
              " 'zitrone': 7797,\n",
              " 'zirkus': 7796,\n",
              " 'zielort': 7795,\n",
              " 'zielfernrohr': 7794,\n",
              " 'ziegelsteingebäude': 7793,\n",
              " 'zerzaust': 7791,\n",
              " 'zerrissenen': 7790,\n",
              " 'zerlumpter': 7789,\n",
              " 'zerlegen': 7788,\n",
              " 'zerkleinert': 7787,\n",
              " 'zerfetzten': 7786,\n",
              " 'zerbrochenen': 7784,\n",
              " 'zerbrochene': 7783,\n",
              " 'zerbricht': 7782,\n",
              " 'zentimeter': 7781,\n",
              " 'zementlaster': 7780,\n",
              " 'zeltplane': 7777,\n",
              " 'zeltet': 7776,\n",
              " 'zehn': 7775,\n",
              " 'zehenspitzen': 7774,\n",
              " 'zaubertrick': 7773,\n",
              " 'zauberer': 7772,\n",
              " 'zange': 7770,\n",
              " 'zahnstocher': 7769,\n",
              " 'zahnarzt': 7768,\n",
              " 'yankees': 7765,\n",
              " 'yamaha': 7764,\n",
              " 'wütend': 7763,\n",
              " 'würzt': 7762,\n",
              " 'würste': 7761,\n",
              " 'wünschen': 7759,\n",
              " 'wälzt': 7758,\n",
              " 'wälder': 7757,\n",
              " 'wrestler': 7754,\n",
              " 'worten': 7753,\n",
              " 'wolke': 7751,\n",
              " 'wohl': 7748,\n",
              " 'wodurch': 7747,\n",
              " 'wirkt': 7746,\n",
              " 'winterlichen': 7744,\n",
              " 'winterbekleidung': 7743,\n",
              " 'winkend': 7742,\n",
              " 'wildwasserrafting': 7741,\n",
              " 'wii': 7739,\n",
              " 'whirlpool': 7736,\n",
              " 'wetteifert': 7734,\n",
              " 'werferhügel': 7730,\n",
              " 'werdendem': 7729,\n",
              " 'wer': 7728,\n",
              " 'wenden': 7727,\n",
              " 'weißhaarige': 7724,\n",
              " 'weiß-braunem': 7721,\n",
              " 'weinflasche': 7718,\n",
              " 'weihnachtsmotiv': 7716,\n",
              " 'wettschwimmen': 7735,\n",
              " 'weiblich': 7712,\n",
              " 'wehrt': 7711,\n",
              " 'wegschaut': 7710,\n",
              " 'weges': 7709,\n",
              " 'wasserstrahl': 7708,\n",
              " 'wasserspritzer': 7707,\n",
              " 'wasserskulptur': 7704,\n",
              " 'wasserrohre': 7702,\n",
              " 'wasseroberfläche': 7700,\n",
              " 'wasserbrunnen': 7697,\n",
              " 'warteraum': 7694,\n",
              " 'wartende': 7693,\n",
              " 'wandgemälde': 7688,\n",
              " 'wandbemalung': 7687,\n",
              " 'waldgegend': 7684,\n",
              " 'wakeboarden': 7681,\n",
              " 'vorübergehend': 7678,\n",
              " 'vorzubereiten': 7676,\n",
              " 'vorstadthaus': 7674,\n",
              " 'vornübergebeugt': 7672,\n",
              " 'vornüber': 7671,\n",
              " 'vornehmen': 7670,\n",
              " 'vornehm': 7669,\n",
              " 'vorgarten': 7667,\n",
              " 'vorbeizukommen': 7664,\n",
              " 'vorbeikommt': 7661,\n",
              " 'volleyballplatz': 7658,\n",
              " 'volles': 7657,\n",
              " 'visier': 7654,\n",
              " 'vierte': 7652,\n",
              " 'vierrädrigen': 7651,\n",
              " 'vierergruppe': 7650,\n",
              " 'vielfältige': 7648,\n",
              " 'vieler': 7647,\n",
              " 'videokameras': 7646,\n",
              " 'videoaufnahme': 7645,\n",
              " 'vesammelt': 7644,\n",
              " 'verzweifelt': 7643,\n",
              " 'verziert': 7642,\n",
              " 'vertreiben': 7638,\n",
              " 'verteidigern': 7636,\n",
              " 'versorgt': 7635,\n",
              " 'verschwommenes': 7633,\n",
              " 'verschmiertem': 7630,\n",
              " 'verrückt': 7626,\n",
              " 'verregneten': 7625,\n",
              " 'vermischt': 7622,\n",
              " 'verlegen': 7620,\n",
              " 'verkäufers': 7619,\n",
              " 'verkäuferin': 7618,\n",
              " 'verkleidung': 7617,\n",
              " 'verkehrsschild': 7615,\n",
              " 'verkaufsautomaten': 7611,\n",
              " 'verhaftet': 7608,\n",
              " 'vergrößerungsglas': 7607,\n",
              " 'vergraben': 7606,\n",
              " 'verglasten': 7605,\n",
              " 'vereinzelten': 7604,\n",
              " 'veranstaltet': 7600,\n",
              " 'vader': 7597,\n",
              " 'urlaub': 7595,\n",
              " 'ureinwohnerin': 7593,\n",
              " 'unterstützen': 7591,\n",
              " 'unterirdischen': 7589,\n",
              " 'unterhosen': 7588,\n",
              " 'unterholz': 7587,\n",
              " 'unterhaltungskünstler': 7586,\n",
              " 'union': 7580,\n",
              " 'unglückliches': 7579,\n",
              " 'ungepflegten': 7577,\n",
              " 'unfallstelle': 7576,\n",
              " 'unfallort': 7575,\n",
              " 'unebenen': 7574,\n",
              " 'unbekannter': 7573,\n",
              " 'unbeholfen': 7571,\n",
              " 'umzugswagen': 7570,\n",
              " 'umzugs': 7569,\n",
              " 'umstehenden': 7568,\n",
              " 'umschlag': 7566,\n",
              " 'umgekippt': 7565,\n",
              " 'umgefallenen': 7564,\n",
              " 'umgebene': 7562,\n",
              " 'umdreht': 7559,\n",
              " 'umarmungen': 7558,\n",
              " 'ufernähe': 7557,\n",
              " 'u-bahn-zug': 7556,\n",
              " 'türkise': 7554,\n",
              " 'tüchern': 7552,\n",
              " 'töpfert': 7551,\n",
              " 'töpfe': 7550,\n",
              " 'töchter': 7549,\n",
              " 'tätowieren': 7547,\n",
              " 'tätigkeiten': 7546,\n",
              " 'turnübung': 7543,\n",
              " 'turnkleidung': 7542,\n",
              " 'turbanen': 7541,\n",
              " 'tuba': 7538,\n",
              " 'trösten': 7537,\n",
              " 'trägershirt': 7536,\n",
              " 'truthähne': 7533,\n",
              " 'zapft': 7771,\n",
              " 'trotzt': 7532,\n",
              " 'tropfen': 7530,\n",
              " 'trockene': 7529,\n",
              " 'trinkgeld': 7527,\n",
              " 'trauung': 7523,\n",
              " 'transport': 7522,\n",
              " 'transparenten': 7521,\n",
              " 'traditionell': 7516,\n",
              " 'toten': 7511,\n",
              " 'topfpflanzen': 7508,\n",
              " 'tollt': 7506,\n",
              " 'tintenfisch': 7502,\n",
              " 'trampelpfad': 7519,\n",
              " 'tim': 7501,\n",
              " 'theaterstück': 7494,\n",
              " 'text': 7492,\n",
              " 'tennisschuh': 7490,\n",
              " 'teilnehmende': 7486,\n",
              " 'teenager-jungen': 7483,\n",
              " 'technologie': 7481,\n",
              " 'teamkollege': 7479,\n",
              " 'tauschen': 7478,\n",
              " 'taubenschwarm': 7477,\n",
              " 'tatort': 7476,\n",
              " 'taste': 7474,\n",
              " 'tarnjacke': 7471,\n",
              " 'tarnfarben': 7469,\n",
              " 'tarnanzügen': 7468,\n",
              " 'tanzstudio': 7466,\n",
              " 'tamburin': 7464,\n",
              " 'takt': 7462,\n",
              " 'taille': 7461,\n",
              " 'süßigkeitenverkäufer': 7458,\n",
              " 'süßer': 7454,\n",
              " 'säuglings': 7452,\n",
              " 'szenerie': 7450,\n",
              " 'surfbrettern': 7448,\n",
              " 'suppe': 7447,\n",
              " 'sumpfiges': 7446,\n",
              " 'sumpfigen': 7445,\n",
              " 'sumpf': 7444,\n",
              " 'stürmischem': 7439,\n",
              " 'vase': 7598,\n",
              " 'stöckelschuhen': 7438,\n",
              " 'stängel': 7437,\n",
              " 'stände': 7436,\n",
              " 'sturm': 7432,\n",
              " 'stuhlreihen': 7430,\n",
              " 'sträuchern': 7427,\n",
              " 'struppiger': 7426,\n",
              " 'strumpfhosen': 7425,\n",
              " 'strommasten': 7424,\n",
              " 'stromleitungen': 7423,\n",
              " 'trittsteinen': 7528,\n",
              " 'striegelt': 7422,\n",
              " 'stretching': 7421,\n",
              " 'streitkräfte': 7420,\n",
              " 'streikschildern': 7418,\n",
              " 'streichorchester': 7417,\n",
              " 'straßenmusik': 7414,\n",
              " 'straßenlauf': 7412,\n",
              " 'straßenkreuzung': 7409,\n",
              " 'straßengeschäft': 7407,\n",
              " 'straßenansicht': 7406,\n",
              " 'strandspaziergang': 7405,\n",
              " 'strandkleid': 7404,\n",
              " 'strandbuggy': 7403,\n",
              " 'strahlendem': 7401,\n",
              " 'stoppschild': 7398,\n",
              " 'stofftieren': 7397,\n",
              " 'stoffen': 7396,\n",
              " 'verwitterten': 7640,\n",
              " 'stocks': 7395,\n",
              " 'stirnlampe': 7394,\n",
              " 'stilles': 7392,\n",
              " 'stiften': 7391,\n",
              " 'sticht': 7389,\n",
              " 'steuern': 7388,\n",
              " 'steuer': 7387,\n",
              " 'sternen': 7384,\n",
              " 'steppdecke': 7383,\n",
              " 'steinweg': 7382,\n",
              " 'steinsäulen': 7381,\n",
              " 'steinstruktur': 7380,\n",
              " 'steinoberfläche': 7378,\n",
              " 'steiniges': 7377,\n",
              " 'steinhaufen': 7376,\n",
              " 'steinbrücke': 7375,\n",
              " 'steinboden': 7374,\n",
              " 'statur': 7371,\n",
              " 'stattfindenden': 7369,\n",
              " 'startbereit': 7365,\n",
              " 'starker': 7364,\n",
              " 'stützrädern': 7442,\n",
              " 'stahlbalken': 7360,\n",
              " 'stahl': 7359,\n",
              " 'stadtteil': 7358,\n",
              " 'stadtplan': 7357,\n",
              " 'stadtbrunnen': 7353,\n",
              " 'stadtarbeiter': 7352,\n",
              " 'spät': 7349,\n",
              " 'sprüngen': 7347,\n",
              " 'sportteam': 7342,\n",
              " 'sportstudio': 7341,\n",
              " 'sportgerät': 7338,\n",
              " 'sportfeld': 7337,\n",
              " 'spirituosengeschäft': 7335,\n",
              " 'spinnkurs': 7333,\n",
              " 'verkaufstand': 7612,\n",
              " 'spießen': 7331,\n",
              " 'spielzeugtraktor': 7330,\n",
              " 'spielzeugs': 7329,\n",
              " 'spielzeugrutsche': 7328,\n",
              " 'spielzeuglastwagen': 7327,\n",
              " 'spielzeughaus': 7325,\n",
              " 'spielzeuggitarre': 7323,\n",
              " 'spielzeugeisenbahn': 7321,\n",
              " 'spieltunnel': 7318,\n",
              " 'spielkonsole': 7316,\n",
              " 'spielkarten': 7315,\n",
              " 'spielhaus': 7314,\n",
              " 'spiderman': 7308,\n",
              " 'speerwurf': 7304,\n",
              " 'sparring': 7302,\n",
              " 'spanischen': 7299,\n",
              " 'space': 7297,\n",
              " 'soße': 7296,\n",
              " 'sonniger': 7293,\n",
              " 'sonnenschirme': 7291,\n",
              " 'sonnenaufgang': 7288,\n",
              " 'sonnenanbetern': 7287,\n",
              " 'sonnebrille': 7286,\n",
              " 'song': 7285,\n",
              " 'sommerkleidung': 7284,\n",
              " 'sommerkleider': 7283,\n",
              " 'soldatinnen': 7282,\n",
              " 'softdrink': 7280,\n",
              " 'sockenhaltern': 7278,\n",
              " 'socke': 7277,\n",
              " 'skimboarden': 7274,\n",
              " 'skifahrerin': 7273,\n",
              " 'skibrille': 7272,\n",
              " 'skianzug': 7271,\n",
              " 'sketch': 7270,\n",
              " 'skateranlage': 7267,\n",
              " 'skateboardfahren': 7266,\n",
              " 'sitzung': 7262,\n",
              " 'sitze': 7261,\n",
              " 'situation': 7259,\n",
              " 'silbernem': 7256,\n",
              " 'signiert': 7254,\n",
              " 'sichtbare': 7252,\n",
              " 'sicherungsseilen': 7251,\n",
              " 'shampoo': 7248,\n",
              " 'set': 7247,\n",
              " 'sessellift': 7246,\n",
              " 'serviette': 7245,\n",
              " 'sense': 7243,\n",
              " 'sendet': 7241,\n",
              " 'seminar': 7240,\n",
              " 'seils': 7237,\n",
              " 'seifenwasser': 7235,\n",
              " 'sehe': 7234,\n",
              " 'verkehrsreiche': 7614,\n",
              " 'seahawks': 7230,\n",
              " 'schürt': 7228,\n",
              " 'schön': 7225,\n",
              " 'schärpe': 7224,\n",
              " 'schäferhunde': 7223,\n",
              " 'schäbig': 7222,\n",
              " 'schwitzender': 7220,\n",
              " 'schwingenden': 7219,\n",
              " 'schwimmkleidung': 7216,\n",
              " 'schwimmhilfen': 7215,\n",
              " 'schwimmhalle': 7214,\n",
              " 'vorhand': 7668,\n",
              " 'schwimmende': 7212,\n",
              " 'schwimmbekleidung': 7211,\n",
              " 'schwelle': 7209,\n",
              " 'schweinefleisch': 7207,\n",
              " 'schwarzweiß': 7206,\n",
              " 'schwarzhaariges': 7205,\n",
              " 'schwarz-orangefarbenen': 7202,\n",
              " 'trainingsanzügen': 7518,\n",
              " 'snap': 7275,\n",
              " 'schutzhelme': 7200,\n",
              " 'schuluniform': 7197,\n",
              " 'schulklasse': 7196,\n",
              " 'schulkindern': 7195,\n",
              " 'schulkantine': 7194,\n",
              " 'schulalter': 7191,\n",
              " 'schuhputzstand': 7190,\n",
              " 'schubst': 7189,\n",
              " 'schubkarren': 7188,\n",
              " 'schrägen': 7187,\n",
              " 'schroffen': 7186,\n",
              " 'schritten': 7185,\n",
              " 'schreienden': 7183,\n",
              " 'schraubenschlüssel': 7180,\n",
              " 'schraubendreher': 7179,\n",
              " 'schottischer': 7178,\n",
              " 'schottenröcken': 7175,\n",
              " 'schnürsenkel': 7173,\n",
              " 'schnüren': 7172,\n",
              " 'schnüffeln': 7171,\n",
              " 'schnurrbärtiger': 7170,\n",
              " 'schnurrbärten': 7169,\n",
              " 'verschiedenfarbiger': 7628,\n",
              " 'schneiderin': 7165,\n",
              " 'schneidbrett': 7164,\n",
              " 'schneesturm': 7163,\n",
              " 'stützen': 7441,\n",
              " 'schneeschuh': 7162,\n",
              " 'träg': 7534,\n",
              " 'schneeschaufel': 7161,\n",
              " 'schnees': 7160,\n",
              " 'schneejacke': 7158,\n",
              " 'schneebrille': 7157,\n",
              " 'schneeausrüstung': 7155,\n",
              " 'schnabel': 7153,\n",
              " 'stadtbus': 7354,\n",
              " 'schmiert': 7152,\n",
              " 'schluckt': 7149,\n",
              " 'schlittschuhe': 7148,\n",
              " 'schlittert': 7147,\n",
              " 'schlips': 7145,\n",
              " 'schlechten': 7141,\n",
              " 'schlechtem': 7140,\n",
              " 'schlangen': 7138,\n",
              " 'schlammiges': 7137,\n",
              " 'schlammigem': 7136,\n",
              " 'schlagmanns': 7135,\n",
              " 'schlafsäcken': 7134,\n",
              " 'schlafanzügen': 7133,\n",
              " 'schirme': 7131,\n",
              " 'schilden': 7128,\n",
              " 'schichten': 7124,\n",
              " 'scheune': 7123,\n",
              " 'scheucht': 7122,\n",
              " 'schaulustiger': 7118,\n",
              " 'schaulustigen': 7117,\n",
              " 'schaukelgestell': 7115,\n",
              " 'schaufensterbummel': 7114,\n",
              " 'saxophonisten': 7109,\n",
              " 'saxophone': 7108,\n",
              " 'saxofonspieler': 7107,\n",
              " 'sausen': 7106,\n",
              " 'sauberen': 7105,\n",
              " 'sanft': 7103,\n",
              " 'sandsturm': 7102,\n",
              " 'sandsack': 7101,\n",
              " 'sandkasten': 7100,\n",
              " 'saiteninstrumenten': 7097,\n",
              " 's': 7096,\n",
              " 'rückbank': 7094,\n",
              " 'rötlichem': 7093,\n",
              " 'röcke': 7092,\n",
              " 'räder': 7091,\n",
              " 'rund': 7089,\n",
              " 'rummelplatz': 7088,\n",
              " 'rot-grünen': 7084,\n",
              " 'rot-gelben': 7081,\n",
              " 'spielzeugfahrrad': 7322,\n",
              " 'rosa-grünen': 7079,\n",
              " 'rollerblader': 7075,\n",
              " 'rodgers': 7073,\n",
              " 'rodeoarena': 7072,\n",
              " 'rodeo-clown': 7071,\n",
              " 'rodeln': 7069,\n",
              " 'rockkonzert': 7066,\n",
              " 'riss': 7061,\n",
              " 'risiko': 7060,\n",
              " 'rinder': 7057,\n",
              " 'richtern': 7055,\n",
              " 'rettungskraft': 7053,\n",
              " 'rettungshelfer': 7052,\n",
              " 'restaurantmitarbeiter': 7050,\n",
              " 'reportern': 7048,\n",
              " 'rennmotorrad': 7045,\n",
              " 'rennhund': 7044,\n",
              " 'renaissance-fest': 7041,\n",
              " 'reißen': 7039,\n",
              " 'süßigkeit': 7456,\n",
              " 'reishut': 7037,\n",
              " 'reisfeld': 7036,\n",
              " 'regennassen': 7031,\n",
              " 'regenjacke': 7029,\n",
              " 'regenbogenfarbigen': 7028,\n",
              " 'reckt': 7026,\n",
              " 'rechteckigen': 7025,\n",
              " 'reaktion': 7024,\n",
              " 'rauen': 7021,\n",
              " 'rauchwolke': 7020,\n",
              " 'rasterlocken': 7019,\n",
              " 'rasten': 7018,\n",
              " 'rasseln': 7017,\n",
              " 'rasiertem': 7016,\n",
              " 'querflöte': 7009,\n",
              " 'päckchen': 7006,\n",
              " 'pusten': 7005,\n",
              " 'pulver': 7002,\n",
              " 'pulk': 7000,\n",
              " 'prächtigen': 6997,\n",
              " 'prächtige': 6996,\n",
              " 'skateboard-kunststück': 7263,\n",
              " 'prächtig': 6995,\n",
              " 'protestierende': 6994,\n",
              " 'professionell': 6992,\n",
              " 'probe': 6990,\n",
              " 'pro': 6989,\n",
              " 'pritschenwagen': 6988,\n",
              " 'presslufthammer': 6986,\n",
              " 'prallen': 6982,\n",
              " 'powerpoint-präsentation': 6980,\n",
              " 'postmitarbeiter': 6978,\n",
              " 'porträts': 6976,\n",
              " 'polo-hemd': 6973,\n",
              " 'polizeifahrzeug': 6971,\n",
              " 'polizeiauto': 6969,\n",
              " 'plüschtieren': 6968,\n",
              " 'platzt': 6966,\n",
              " 'plattenladen': 6965,\n",
              " 'plastikeimer': 6961,\n",
              " 'plakate': 6959,\n",
              " 'pitbull': 6958,\n",
              " 'schreibend': 7182,\n",
              " 'pink-weißem': 6957,\n",
              " 'pinien': 6956,\n",
              " 'pinguine': 6955,\n",
              " 'pilz': 6954,\n",
              " 'stämmiger': 7435,\n",
              " 'pickup-truck': 6952,\n",
              " 'piano': 6951,\n",
              " 'pfote': 6949,\n",
              " 'pferdewagen': 6946,\n",
              " 'pferdeschwänzen': 6945,\n",
              " 'pferdefuhrwerk': 6944,\n",
              " 'pferch': 6943,\n",
              " 'pfauenkostüm': 6941,\n",
              " 'pfau': 6940,\n",
              " 'pfarrer': 6939,\n",
              " 'rolltreppen': 7077,\n",
              " 'perücken': 6936,\n",
              " 'personenzug': 6935,\n",
              " 'penn': 6932,\n",
              " 'pelikan': 6930,\n",
              " 'pedale': 6929,\n",
              " 'pavillon': 6927,\n",
              " 'patrouilliert': 6926,\n",
              " 'patriotischen': 6925,\n",
              " 'pastellfarbenen': 6923,\n",
              " 'passieren': 6922,\n",
              " 'passender': 6921,\n",
              " 'passendem': 6920,\n",
              " 'u-bahn-bahnsteig': 7555,\n",
              " 'passagier': 6918,\n",
              " 'parkuhr': 6914,\n",
              " 'parka': 6912,\n",
              " 'pappschachtel': 6909,\n",
              " 'papierteller': 6908,\n",
              " 'papagei': 6906,\n",
              " 'palmwedeln': 6905,\n",
              " 'pakete': 6904,\n",
              " 'paintball': 6903,\n",
              " 'pain': 6902,\n",
              " 'pabst': 6901,\n",
              " 'outdoor-bekleidung': 6900,\n",
              " 'out': 6899,\n",
              " 'orten': 6898,\n",
              " 'orioles': 6897,\n",
              " 'orgel': 6896,\n",
              " 'organgefarbenen': 6894,\n",
              " 'orange-grauen': 6890,\n",
              " 'orange-blauen': 6889,\n",
              " 'opfer': 6888,\n",
              " 'open-air-konzert': 6886,\n",
              " 'olympische': 6884,\n",
              " 'ohrringen': 6881,\n",
              " 'ohrhörer': 6880,\n",
              " 'objektiv': 6878,\n",
              " 'oberleitungsfahrzeug': 6877,\n",
              " 'obama': 6876,\n",
              " 'oakland': 6875,\n",
              " 'nächtliches': 6873,\n",
              " 'notre': 6869,\n",
              " 'notiz': 6868,\n",
              " 'notenblättern': 6867,\n",
              " 'notenblatt': 6866,\n",
              " 'normaler': 6862,\n",
              " 'no': 6860,\n",
              " 'nische': 6859,\n",
              " 'niemand': 6858,\n",
              " 'niedrig': 6857,\n",
              " 'new-york-hut': 6856,\n",
              " 'neugieriges': 6855,\n",
              " 'neugeborenen': 6854,\n",
              " 'neugeborene': 6853,\n",
              " 'neongrün': 6851,\n",
              " 'neongelben': 6850,\n",
              " 'neonfarbenen': 6849,\n",
              " 'namensschildern': 6844,\n",
              " 'tribünen': 7526,\n",
              " 'nahrung': 6842,\n",
              " 'nadelstreifenhemd': 6840,\n",
              " 'nacken': 6839,\n",
              " 'nachrichtensprecher': 6838,\n",
              " 'nachricht': 6837,\n",
              " 'nachgehen': 6835,\n",
              " 'nachdenklich': 6834,\n",
              " 'nachbarn': 6832,\n",
              " 'mürrisch': 6831,\n",
              " 'münztelefon': 6830,\n",
              " 'münzfernsprecher': 6829,\n",
              " 'mülltonnen': 6828,\n",
              " 'möglich': 6826,\n",
              " 'möbelgeschäft': 6825,\n",
              " 'männerteam': 6824,\n",
              " 'mädchenmannschaften': 6822,\n",
              " 'mädchengruppe': 6821,\n",
              " 'mustang': 6820,\n",
              " 'muslimischer': 6819,\n",
              " 'typischen': 7545,\n",
              " 'muslimische': 6818,\n",
              " 'musikerin': 6815,\n",
              " 'muffins': 6812,\n",
              " 'mountainbike-fahrer': 6811,\n",
              " 'motorschlitten': 6810,\n",
              " 'motorrads': 6809,\n",
              " 'motorradhelm': 6807,\n",
              " 'motorradfahrerin': 6806,\n",
              " 'motorradfahren': 6805,\n",
              " 'molliges': 6799,\n",
              " 'modischer': 6796,\n",
              " 'moderner': 6793,\n",
              " 'modelliert': 6792,\n",
              " 'mittelpunkt': 6788,\n",
              " 'mittelgroße': 6787,\n",
              " 'mitgliedern': 6786,\n",
              " 'mitfahrgelegenheit': 6785,\n",
              " 'mischen': 6784,\n",
              " 'minnesota': 6782,\n",
              " 'minirock': 6781,\n",
              " 'militärischen': 6780,\n",
              " 'militärhosen': 6779,\n",
              " 'mikroskopen': 6778,\n",
              " 'mike': 6777,\n",
              " 'michigan': 6775,\n",
              " 'michael': 6774,\n",
              " 'mexiko': 6773,\n",
              " 'metzgerei': 6772,\n",
              " 'metallstruktur': 6768,\n",
              " 'metallrahmen': 6765,\n",
              " 'metallgerüst': 6762,\n",
              " 'metalldach': 6759,\n",
              " 'metallbauwerk': 6758,\n",
              " 'metallarbeiter': 6756,\n",
              " 'menschliche': 6754,\n",
              " 'menschenleeren': 6752,\n",
              " 'meißel': 6751,\n",
              " 'mehrfarbiger': 6749,\n",
              " 'meereswellen': 6747,\n",
              " 'meeresfrüchte': 6745,\n",
              " 'matrosenanzug': 6742,\n",
              " 'matrosen': 6741,\n",
              " 'maskierter': 6736,\n",
              " 'marmor': 6734,\n",
              " 'marker': 6732,\n",
              " 'marineblauer': 6731,\n",
              " 'marineblau': 6730,\n",
              " 'marathons': 6729,\n",
              " 'marathonläufern': 6728,\n",
              " 'manns': 6727,\n",
              " 'mandel': 6725,\n",
              " 'transformator': 7520,\n",
              " 'mammutbaum': 6724,\n",
              " 'malerischen': 6723,\n",
              " 'majestätischen': 6721,\n",
              " 'mahl': 6719,\n",
              " 'sumoringer': 7443,\n",
              " 'magazin': 6718,\n",
              " 'mach': 6717,\n",
              " 'mac-computer': 6716,\n",
              " 'm': 6715,\n",
              " 'löwenzahnblüte': 6713,\n",
              " 'löffeln': 6712,\n",
              " 'lätzchen': 6711,\n",
              " 'ländliche': 6707,\n",
              " 'lutscht': 6706,\n",
              " 'lustiges': 6704,\n",
              " 'lungern': 6702,\n",
              " 'luftmatratzen': 6701,\n",
              " 'luftballontiere': 6700,\n",
              " 'louvre': 6699,\n",
              " 'londoner': 6698,\n",
              " 'lolli': 6697,\n",
              " 'lokomotiven': 6696,\n",
              " 'lokomotive': 6695,\n",
              " 'livekonzert': 6693,\n",
              " 'linsen': 6692,\n",
              " 'linkshändiger': 6690,\n",
              " 'linkes': 6689,\n",
              " 'notebooks': 6865,\n",
              " 'lineal': 6688,\n",
              " 'limonaden': 6687,\n",
              " 'lilane': 6685,\n",
              " 'schulen': 7192,\n",
              " 'pedal': 6928,\n",
              " 'lilafarbenem': 6684,\n",
              " 'lieben': 6679,\n",
              " 'lichtung': 6678,\n",
              " 'lichts': 6677,\n",
              " 'leuchtturm': 6676,\n",
              " 'leuchtreklame': 6675,\n",
              " 'leuchtender': 6674,\n",
              " 'leuchtendem': 6673,\n",
              " 'lesenden': 6672,\n",
              " 'leitung': 6669,\n",
              " 'leitplanken': 6668,\n",
              " 'leht': 6667,\n",
              " 'lehnstuhl': 6666,\n",
              " 'leerstehenden': 6665,\n",
              " 'lederoberteil': 6660,\n",
              " 'lebron': 6657,\n",
              " 'lebensmittelbereich': 6654,\n",
              " 'spachtel': 7298,\n",
              " 'lebens': 6653,\n",
              " 'lebendigen': 6652,\n",
              " 'leadsänger': 6651,\n",
              " 'lautsprechern': 6649,\n",
              " 'laut': 6648,\n",
              " 'laufstrecke': 6646,\n",
              " 'laufschuhen': 6645,\n",
              " 'laufoberteil': 6644,\n",
              " 'laufhosen': 6642,\n",
              " 'laufgitter': 6641,\n",
              " 'laufenden': 6640,\n",
              " 'laubwerk': 6639,\n",
              " 'laternenpfahl': 6634,\n",
              " 'lateinamerikaner': 6632,\n",
              " 'las': 6631,\n",
              " 'langgeht': 6626,\n",
              " 'lang': 6625,\n",
              " 'landwirtschaftlichen': 6624,\n",
              " 'stürmt': 7440,\n",
              " 'landwirt': 6623,\n",
              " 'lakers': 6620,\n",
              " 'tiers': 7500,\n",
              " 'laken': 6619,\n",
              " 'lagerraum': 6618,\n",
              " 'lagerhalle': 6617,\n",
              " 'ladengeschäft': 6616,\n",
              " 'lachenden': 6611,\n",
              " 'labradoodle': 6610,\n",
              " 'künstlerischen': 6603,\n",
              " 'künstlerische': 6602,\n",
              " 'kühlboxen': 6600,\n",
              " 'kühlbox': 6599,\n",
              " 'kühe': 6598,\n",
              " 'küchenutensilien': 6597,\n",
              " 'küchenmaschine': 6596,\n",
              " 'kurzes': 6593,\n",
              " 'kurven': 6592,\n",
              " 'kunstvoll': 6591,\n",
              " 'kunststofftasse': 6590,\n",
              " 'kunststofffolie': 6589,\n",
              " 'kunstprojekt': 6587,\n",
              " 'kulturellen': 6583,\n",
              " 'kultur': 6582,\n",
              " 'kugelbad': 6581,\n",
              " 'krügen': 6580,\n",
              " 'kreuzen': 6576,\n",
              " 'kreativ': 6574,\n",
              " 'krankenpfleger': 6573,\n",
              " 'krankenhauskleidung': 6572,\n",
              " 'kostümierter': 6571,\n",
              " 'kostenlose': 6570,\n",
              " 'kosten': 6569,\n",
              " 'korpulentes': 6568,\n",
              " 'korken': 6567,\n",
              " 'konserven': 6560,\n",
              " 'nuckelt': 6870,\n",
              " 'kongress': 6559,\n",
              " 'kommunikationsgerät': 6558,\n",
              " 'komisches': 6557,\n",
              " 'komischer': 6556,\n",
              " 'kochmütze': 6551,\n",
              " 'knöpfe': 6549,\n",
              " 'knietief': 6546,\n",
              " 'kniestrümpfe': 6545,\n",
              " 'kniender': 6544,\n",
              " 'knapp': 6542,\n",
              " 'klub': 6540,\n",
              " 'klippen': 6539,\n",
              " 'kleinkinds': 6536,\n",
              " 'kleinbusses': 6535,\n",
              " 'kleidungsstück': 6534,\n",
              " 'konzertaufführung': 6561,\n",
              " 'klassischen': 6533,\n",
              " 'kitzelt': 6531,\n",
              " 'kirmesattraktion': 6526,\n",
              " 'kirchenchor': 6525,\n",
              " 'kippt': 6524,\n",
              " 'kinderschaukel': 6520,\n",
              " 'kinderbuch': 6519,\n",
              " 'kimono': 6517,\n",
              " 'kiesstrand': 6516,\n",
              " 'kieshügel': 6515,\n",
              " 'kieselsteine': 6514,\n",
              " 'khaki-hosen': 6513,\n",
              " 'kettenkarussell': 6512,\n",
              " 'kennzeichnet': 6510,\n",
              " 'keks': 6509,\n",
              " 'kegelförmigen': 6507,\n",
              " 'kawasaki': 6506,\n",
              " 'waschmaschinen': 7696,\n",
              " 'kauen': 6505,\n",
              " 'katzenbaby': 6504,\n",
              " 'katzenbabies': 6503,\n",
              " 'kastanienbraunes': 6502,\n",
              " 'kasino': 6501,\n",
              " 'kartoffelchip': 6500,\n",
              " 'karge': 6499,\n",
              " 'karatekampf': 6497,\n",
              " 'karate-klasse': 6496,\n",
              " 'kaputter': 6495,\n",
              " 'kampfsportturnier': 6491,\n",
              " 'kampfanzügen': 6490,\n",
              " 'kameracrew': 6488,\n",
              " 'kaltem': 6487,\n",
              " 'kais': 6485,\n",
              " 'life': 6681,\n",
              " 'kahlrasiertem': 6484,\n",
              " 'kaffeehaus': 6482,\n",
              " 'jüdischer': 6480,\n",
              " 'jüdischen': 6479,\n",
              " 'jüdische': 6478,\n",
              " 'juweliergeschäft': 6477,\n",
              " 'jump': 6474,\n",
              " 'joe': 6472,\n",
              " 'jim': 6471,\n",
              " 'jerusalem': 6470,\n",
              " 'jede': 6467,\n",
              " 'jeans-shorts': 6465,\n",
              " 'japanisches': 6464,\n",
              " 'jahr': 6459,\n",
              " 'italien': 6458,\n",
              " 'irokesenhaarschnitt': 6457,\n",
              " 'irgendwohin': 6456,\n",
              " 'irgendwelche': 6455,\n",
              " 'irgendeines': 6454,\n",
              " 'polizeibeamter': 6970,\n",
              " 'ipad': 6452,\n",
              " 'iowa': 6451,\n",
              " 'interessant': 6449,\n",
              " 'insekten': 6448,\n",
              " 'innenstadtbereich': 6445,\n",
              " 'innenräumen': 6444,\n",
              " 'soda': 7279,\n",
              " 'inline-skater': 6442,\n",
              " 'informellen': 6441,\n",
              " 'informell': 6440,\n",
              " 'inderinnen': 6438,\n",
              " 'immer': 6437,\n",
              " 'schwimmwettkampf': 7218,\n",
              " 'imbisswagen': 6436,\n",
              " 'imbiss': 6434,\n",
              " 'iglu': 6433,\n",
              " 'säcke': 7451,\n",
              " 'ice': 6432,\n",
              " 'hütten': 6431,\n",
              " 'hütet': 6430,\n",
              " 'hütchen': 6429,\n",
              " 'hündin': 6428,\n",
              " 'verbirgt': 7602,\n",
              " 'hühner': 6427,\n",
              " 'fahrradreifen': 3204,\n",
              " 'und': 10,\n",
              " 'touristin': 7513,\n",
              " 'elternteil': 3193,\n",
              " 'eishockeyspiel': 3190,\n",
              " 'durchgang': 3184,\n",
              " 'dicker': 3179,\n",
              " 'derby': 3176,\n",
              " 'unterhaltung': 4177,\n",
              " 'deckt': 3175,\n",
              " 'collie': 3173,\n",
              " 'hinauf': 396,\n",
              " 'dieselbe': 2279,\n",
              " 'gericht': 2314,\n",
              " 'brät': 3164,\n",
              " 'buchladen': 5747,\n",
              " 'brechen': 3162,\n",
              " 'parkgarage': 6913,\n",
              " 'blätterhaufen': 3159,\n",
              " 'gemauerten': 6183,\n",
              " 'blond': 3155,\n",
              " 'erdboden': 3196,\n",
              " 'männe': 6823,\n",
              " 'bewundern': 3151,\n",
              " 'geschminkt': 2848,\n",
              " 'gürtel': 1821,\n",
              " 'bekleidungsgeschäft': 3143,\n",
              " 'diesem': 930,\n",
              " 'turnübungen': 7544,\n",
              " 'begleiter': 3142,\n",
              " 'wasserfall': 1053,\n",
              " 'kleinkind': 299,\n",
              " 'bass': 3134,\n",
              " 'bambus': 3130,\n",
              " 'stadtpark': 2411,\n",
              " 'babyschaukel': 3123,\n",
              " 'feldhockey': 3206,\n",
              " 'wobei': 601,\n",
              " 'kabinen': 6481,\n",
              " 'außerdem': 3122,\n",
              " 'erwachsene': 330,\n",
              " '6': 3102,\n",
              " 'jongleur': 6473,\n",
              " 'schuhputzer': 5059,\n",
              " 'ausflug': 3118,\n",
              " 'waldlandschaft': 7685,\n",
              " 'erzählt': 3202,\n",
              " 'gymnastik': 3255,\n",
              " 'ans': 3108,\n",
              " 'rücksitz': 2008,\n",
              " 'entlanggehen': 3702,\n",
              " 'gespielt': 2316,\n",
              " 'zeitungen': 3090,\n",
              " 'ansammlung': 3109,\n",
              " 'zebrastreifen': 3089,\n",
              " 'süßigkeitenladen': 7457,\n",
              " 'drauf': 5861,\n",
              " 'festival': 3213,\n",
              " 'wände': 3087,\n",
              " 'weißhaariger': 3079,\n",
              " 'weiß-schwarzer': 3078,\n",
              " 'wandmalerei': 3072,\n",
              " 'anzeige': 4288,\n",
              " 'vorbeifahren': 3069,\n",
              " 'spielzeughammer': 7324,\n",
              " 'verärgert': 3064,\n",
              " 'schirmt': 7132,\n",
              " 'verfolgen': 3059,\n",
              " 'umgestürzten': 3054,\n",
              " 'handtasche': 912,\n",
              " 'windjacke': 4217,\n",
              " 'türkisfarbenen': 3052,\n",
              " 'tische': 3046,\n",
              " 'gerichtet': 2847,\n",
              " 'tennisschuhen': 3042,\n",
              " 'badebekleidung': 3604,\n",
              " 'betten': 5659,\n",
              " 'teilnimmt': 3040,\n",
              " 'straßenszene': 2417,\n",
              " 'teenagerin': 3039,\n",
              " 'lötet': 3913,\n",
              " 'stöbern': 3035,\n",
              " 'japanischen': 1831,\n",
              " 'turnschuhe': 5216,\n",
              " 'studiert': 3034,\n",
              " 'musikgruppe': 2161,\n",
              " 'stützt': 3473,\n",
              " 'einhändigen': 5903,\n",
              " 'seitenlinie': 3015,\n",
              " 'seilspringen': 3014,\n",
              " 'schwimmflügeln': 3010,\n",
              " 'rennautos': 4986,\n",
              " 'schwarz-braunen': 3006,\n",
              " 'schwarz-braune': 3005,\n",
              " 'schmutziges': 2999,\n",
              " 'kopfhörern': 1187,\n",
              " 'schachbrett': 7110,\n",
              " 'schmetterling': 2997,\n",
              " 'schlittschuh': 2996,\n",
              " 'stolpert': 3467,\n",
              " 'schlafzimmer': 2993,\n",
              " 'touristengruppe': 5201,\n",
              " 'saiteninstrument': 2988,\n",
              " 'sein': 132,\n",
              " 'ruhigem': 2986,\n",
              " 'fressen': 6097,\n",
              " 'ruft': 2985,\n",
              " 'fahrradrahmen': 5987,\n",
              " 'rot-weißes': 2983,\n",
              " 'rot-schwarzer': 2981,\n",
              " 'nassgespritzt': 6845,\n",
              " 'einkäufer': 2524,\n",
              " 'rennfahrer': 2979,\n",
              " 'raufen': 2978,\n",
              " 'rasenmäher': 2976,\n",
              " 'genüsslich': 3770,\n",
              " 'quad': 2974,\n",
              " 'drängen': 3180,\n",
              " 'backsteinwand': 3124,\n",
              " 'pyramide': 2973,\n",
              " 'puppen': 2972,\n",
              " 'gasmaske': 4574,\n",
              " 'punkten': 2971,\n",
              " 'produkt': 2968,\n",
              " 'popcorn': 2965,\n",
              " 'stroh': 3471,\n",
              " 'rentier': 7046,\n",
              " 'luftballons': 1242,\n",
              " 'pflückt': 2960,\n",
              " 'schnurrbart': 1197,\n",
              " 'frankreich': 4552,\n",
              " 'schlittenhunde': 7146,\n",
              " 'holzstamm': 6402,\n",
              " 'brotstand': 5739,\n",
              " 'ochsen': 2954,\n",
              " 'samen': 7098,\n",
              " 'industriegebäude': 4727,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "SRC.vocab.get_stoi()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4PiQ3HR9WKP"
      },
      "source": [
        "* 한 문장에 포함된 단어가 연속적으로 **LSTM**에 입력되어야 합니다.\n",
        "    * 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋습니다.\n",
        "    * 이를 위해 BucketIterator를 사용합니다.\n",
        "    * **배치 크기(batch size)**: 128"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    de_list,en_list=[],[]\n",
        "    for (_de,_en) in batch:\n",
        "        de_list.append(torch.tensor(SRC([sos_token]+tokenize_de(_de.lower())+[eos_token]),dtype=torch.int64))\n",
        "        en_list.append(torch.tensor(TRG([sos_token]+tokenize_en(_en.lower())+[eos_token]),dtype=torch.int64))\n",
        "    de_inputs = torch.nn.utils.rnn.pad_sequence(de_list, batch_first=True,padding_value=1)\n",
        "    en_inputs = torch.nn.utils.rnn.pad_sequence(en_list, batch_first=True,padding_value=1)\n",
        "    return (de_inputs.contiguous(),en_inputs.contiguous())"
      ],
      "metadata": {
        "id": "eWyQu21W1WJa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KM6EVV2t9BHd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator,test_iterator,valid_iterator = iter(Multi30k(root='.data', split=('train','test','valid'), language_pair=(\"de\", \"en\")))\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iterator, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_iterator, batch_size=BATCH_SIZE,collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " for (i,j) in train_dataloader:\n",
        "   print(i)\n",
        "   print(i.shape)\n",
        "   print(j)\n",
        "   print(j.shape)\n",
        "   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVOI24F73MBK",
        "outputId": "4e97db46-f645-4e54-a433-e6d2260134b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2,    4,  135,  ...,    1,    1,    1],\n",
            "        [   2,    4,  224,  ...,    1,    1,    1],\n",
            "        [   2,    4,  864,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,    4, 3199,  ...,    1,    1,    1],\n",
            "        [   2,    4,  746,  ...,    1,    1,    1],\n",
            "        [   2,    4,  495,  ...,    1,    1,    1]])\n",
            "torch.Size([128, 28])\n",
            "tensor([[  2,   4, 123,  ...,   1,   1,   1],\n",
            "        [  2,   4,   9,  ...,   1,   1,   1],\n",
            "        [  2,  16,  30,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [  2,  16,  30,  ...,   1,   1,   1],\n",
            "        [  2,   4,  61,  ...,   1,   1,   1],\n",
            "        [  2,   4,  38,  ...,   1,   1,   1]])\n",
            "torch.Size([128, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " cnt=0\n",
        " a=[]\n",
        " for i, batch in enumerate(train_iterator):\n",
        "     a.append(batch)\n",
        "     print(i)\n",
        "     print(batch)\n",
        "     cnt+=1\n",
        "     if cnt>4:\n",
        "         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odDC7RId3MHV",
        "outputId": "118c1315-07aa-411d-e85f-ec0d637d3491"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "('Ein Gruppe von Freunden mit Gesichtsfarbe und Federstirnbändern sitzt auf einer Bank.', 'A young group of friends adorned with face paint and feathered headbands sit on a bench.')\n",
            "1\n",
            "('Ein weiß gekleidetes Kleinkind lächelt, während eine Dame ihm hilft, eine Flagge zu schwenken.', 'A little toddler dressed in white is smiling while a lady helps him wave a flag.')\n",
            "2\n",
            "('Ein Mann in schwarzem Leder-Motoarradanzug und mit Helm auf einem silbernen Motorrad.', 'A guy in a black leather motorcycle suit an helmet on a silver motorcycle.')\n",
            "3\n",
            "('Ein Jockey auf einem Pferd versucht, über Holzteile auf einem Platz.', 'A jockey on a horse is attempting to jump logs in a field.')\n",
            "4\n",
            "('Ein Mann hebt schwere Steine hoch und stapelt sie übereinander.', 'A man is lifting heavy rocks and stacking them on top of one another.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qgg7DjP_3MO-",
        "outputId": "db1ff1bc-8296-48ec-d715-be444cf015c9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ein Gruppe von Freunden mit Gesichtsfarbe und Federstirnbändern sitzt auf einer Bank.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " a[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bXFGEnMa3brR",
        "outputId": "8f603fa6-c4da-4494-aa85-485ed973c3dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A young group of friends adorned with face paint and feathered headbands sit on a bench.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oguDx24e-3Db"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 주어진 소스 문장을 **문맥 벡터(context vector)로 인코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ac28d5DL_ceY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 인코더(Encoder) 아키텍처 정의\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n",
        "    def forward(self, src):\n",
        "        # src: [단어 개수, 배치 크기]: 각 단어의 인덱스(index) 정보\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [단어 개수, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 문맥 벡터(context vector) 반환\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pHj4IlvBzPe"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 주어진 문맥 벡터(context vector)를 **타겟 문장으로 디코딩**합니다.\n",
        "* LSTM은 hidden state과 cell state을 반환합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **hidden_dim**: 히든 상태(hidden state) 차원\n",
        "    * **n_layers**: RNN 레이어의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fusf6_9yDmfM"
      },
      "outputs": [],
      "source": [
        "# 디코더(Decoder) 아키텍처 정의\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # FC 레이어 (인코더와 구조적으로 다른 부분)\n",
        "        self.output_dim = output_dim\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        # cell = context: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input: [단어 개수 = 1, 배치 크기]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output: [단어 개수 = 1, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 단어 개수는 어차피 1개이므로 차원 제거\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [배치 크기, 출력 차원]\n",
        "        \n",
        "        # (현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17WxtpJI_9V"
      },
      "source": [
        "#### **Seq2Seq 아키텍처**\n",
        "\n",
        "* 앞서 정의한 인코더(encoder)와 디코더(decoder)를 가지고 있는 하나의 아키텍처입니다.\n",
        "    * **인코더(encoder)**: 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩합니다.\n",
        "    * **디코더(decoder)**: 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
        "    * 단, **디코더는 한 단어씩** 넣어서 한 번씩 결과를 구합니다.\n",
        "* **Teacher forcing**: 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vsA6C6B5Glhc"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [단어 개수, 배치 크기]\n",
        "        # trg: [단어 개수, 배치 크기]\n",
        "        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
        "        trg_len = trg.shape[0] # 단어 개수\n",
        "        batch_size = trg.shape[1] # 배치 크기\n",
        "        trg_vocab_size = self.decoder.output_dim # 출력 차원\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 첫 번째 입력은 항상 <sos> 토큰\n",
        "        input = trg[0, :]\n",
        "\n",
        "        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
        "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
        "\n",
        "            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyXRWyDrHYSB"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qVsGIVvzMZ-N"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENCODER_EMBED_DIM = 256\n",
        "DECODER_EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT_RATIO = 0.5\n",
        "DEC_DROPOUT_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0WM3urPiIE1T"
      },
      "outputs": [],
      "source": [
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n",
        "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n",
        "\n",
        "# Seq2Seq 객체 선언\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnWnXGIMwHk"
      },
      "source": [
        "* 논문의 내용대로 $\\mathcal{U}(-0.08, 0.08)$의 값으로 **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Zdfqma4uMaoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8666336d-2f9b-4a5e-881c-4500132d8aa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7853, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUwj0UgIS6E"
      },
      "source": [
        "* 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BeqqI7xfM71V"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "TRG_PAD_IDX = TRG[\"<pad>\"]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SXo7ZOclNG2-"
      },
      "outputs": [],
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "    count=0\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for src,trg in iterator:\n",
        "        src = torch.transpose(src,0,1).to(device)\n",
        "        trg = torch.transpose(trg,0,1).to(device)\n",
        "        count+=1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "        \n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    \n",
        "    return epoch_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DzR8hm9HQ1gb"
      },
      "outputs": [],
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "    count=0\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for src,trg in iterator:\n",
        "            src = torch.transpose(src,0,1).to(device)\n",
        "            trg = torch.transpose(trg,0,1).to(device)\n",
        "            count+=1\n",
        "            # 평가할 때 teacher forcing는 사용하지 않음\n",
        "            output = model(src, trg, 0)\n",
        "            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76X7pR1cLtAl"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "iVGWe9VtSwx0"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EMMkAnGeSyMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f3541f-5966-440c-8c37-8ad3039dc457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 36s\n",
            "\tTrain Loss: 4.513 | Train PPL: 91.170\n",
            "\tValidation Loss: 4.749 | Validation PPL: 115.514\n",
            "Epoch: 02 | Time: 0m 37s\n",
            "\tTrain Loss: 4.178 | Train PPL: 65.257\n",
            "\tValidation Loss: 4.602 | Validation PPL: 99.727\n",
            "Epoch: 03 | Time: 0m 36s\n",
            "\tTrain Loss: 3.980 | Train PPL: 53.520\n",
            "\tValidation Loss: 4.482 | Validation PPL: 88.398\n",
            "Epoch: 04 | Time: 0m 36s\n",
            "\tTrain Loss: 3.829 | Train PPL: 46.002\n",
            "\tValidation Loss: 4.333 | Validation PPL: 76.152\n",
            "Epoch: 05 | Time: 0m 36s\n",
            "\tTrain Loss: 3.659 | Train PPL: 38.814\n",
            "\tValidation Loss: 4.232 | Validation PPL: 68.840\n",
            "Epoch: 06 | Time: 0m 36s\n",
            "\tTrain Loss: 3.544 | Train PPL: 34.594\n",
            "\tValidation Loss: 4.147 | Validation PPL: 63.222\n",
            "Epoch: 07 | Time: 0m 36s\n",
            "\tTrain Loss: 3.388 | Train PPL: 29.619\n",
            "\tValidation Loss: 4.065 | Validation PPL: 58.259\n",
            "Epoch: 08 | Time: 0m 36s\n",
            "\tTrain Loss: 3.229 | Train PPL: 25.248\n",
            "\tValidation Loss: 3.996 | Validation PPL: 54.394\n",
            "Epoch: 09 | Time: 0m 36s\n",
            "\tTrain Loss: 3.110 | Train PPL: 22.420\n",
            "\tValidation Loss: 3.899 | Validation PPL: 49.355\n",
            "Epoch: 10 | Time: 0m 36s\n",
            "\tTrain Loss: 3.002 | Train PPL: 20.133\n",
            "\tValidation Loss: 3.813 | Validation PPL: 45.283\n",
            "Epoch: 11 | Time: 0m 36s\n",
            "\tTrain Loss: 2.888 | Train PPL: 17.953\n",
            "\tValidation Loss: 3.862 | Validation PPL: 47.583\n",
            "Epoch: 12 | Time: 0m 36s\n",
            "\tTrain Loss: 2.803 | Train PPL: 16.499\n",
            "\tValidation Loss: 3.776 | Validation PPL: 43.660\n",
            "Epoch: 13 | Time: 0m 36s\n",
            "\tTrain Loss: 2.684 | Train PPL: 14.651\n",
            "\tValidation Loss: 3.809 | Validation PPL: 45.114\n",
            "Epoch: 14 | Time: 0m 36s\n",
            "\tTrain Loss: 2.608 | Train PPL: 13.570\n",
            "\tValidation Loss: 3.766 | Validation PPL: 43.207\n",
            "Epoch: 15 | Time: 0m 36s\n",
            "\tTrain Loss: 2.528 | Train PPL: 12.525\n",
            "\tValidation Loss: 3.752 | Validation PPL: 42.592\n",
            "Epoch: 16 | Time: 0m 36s\n",
            "\tTrain Loss: 2.433 | Train PPL: 11.397\n",
            "\tValidation Loss: 3.748 | Validation PPL: 42.454\n",
            "Epoch: 17 | Time: 0m 36s\n",
            "\tTrain Loss: 2.386 | Train PPL: 10.867\n",
            "\tValidation Loss: 3.675 | Validation PPL: 39.433\n",
            "Epoch: 18 | Time: 0m 37s\n",
            "\tTrain Loss: 2.306 | Train PPL: 10.033\n",
            "\tValidation Loss: 3.669 | Validation PPL: 39.224\n",
            "Epoch: 19 | Time: 0m 37s\n",
            "\tTrain Loss: 2.257 | Train PPL: 9.553\n",
            "\tValidation Loss: 3.692 | Validation PPL: 40.136\n",
            "Epoch: 20 | Time: 0m 36s\n",
            "\tTrain Loss: 2.167 | Train PPL: 8.729\n",
            "\tValidation Loss: 3.758 | Validation PPL: 42.871\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvYlZ85ZRUya"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델 저장\n",
        "from google.colab import files\n",
        "\n",
        "files.download('seq2seq.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6d2isoPL_P0"
      },
      "source": [
        "#### **모델 최종 테스트(testing) 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "co3YMQ2NS0Ia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9a7cbfd3-5ceb-4a1a-9492-02df96aec411"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-27ebaa602b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seq2seq.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([7855, 256]) from checkpoint, the shape in current model is torch.Size([7853, 256])."
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('seq2seq.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMvcdNR7YqAo"
      },
      "source": [
        "#### **나만의 데이터로 모델 사용해보기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "OjHJkeznS1oS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "54beec16-0d87-4b88-e62c-2eb016d5d18c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-5611fb0e84c7>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    return trg_tokens[1:]출력 문장 반환\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load(\"de_core_news_sm\")\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [sos_token] +tokens + [eos_token]\n",
        "    print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = src_field(tokens)\n",
        "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field[sos_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == trg_field[eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = trg_field.lookup_tokens(trg_indexes)\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]출력 문장 반환\n",
        "    return trg_tokens[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1OSgbkh0Vkq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "66ff38ad-0bfd-4770-87a5-8331b854382c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-07fca0cad526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_dataset.examples[example_idx])['src']\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR4mSubPXOJV"
      },
      "outputs": [],
      "source": [
        "src = tokenize_de(\"Guten Abend.\")\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}