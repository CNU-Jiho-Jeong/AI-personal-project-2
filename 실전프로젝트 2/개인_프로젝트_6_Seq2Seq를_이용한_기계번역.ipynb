{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1buQp2iaXx"
      },
      "source": [
        "#### **Sequence to Sequence Learning with Neural Networks (NIPS 2014)** 실습\n",
        "* 본 코드는 기본적으로 <b>Seq2Seq 논문</b>의 내용을 최대한 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델은 Seq2Seq with Attention이 아닌 **Transformer 기반의 모델**입니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V6X0LxZitih"
      },
      "source": [
        "#### **데이터 전처리(Preprocessing)**\n",
        "\n",
        "* **spaCy 라이브러리**: 문장의 토큰화(tokenization), 태깅(tagging) 등의 전처리 기능을 위한 라이브러리\n",
        "  * 영어(Engilsh)와 독일어(Deutsch) 전처리 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0umI87bziUih"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\") # 영어 토큰화(tokenization)\n",
        "spacy_de = spacy.load(\"de_core_news_sm\") # 독일어 토큰화(tokenization)"
      ],
      "metadata": {
        "id": "g6CyVzubl_vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWyGCWfivbO",
        "outputId": "88830543-8beb-4237-de2c-601b1539827e"
      },
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmA4kX0QixwZ"
      },
      "source": [
        "* 영어(English) 및 독일어(Deutsch) **토큰화 함수** 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-pEypzUiwnX"
      },
      "source": [
        "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수\n",
        "def tokenize_de(text):\n",
        "    return [token.text.lower() for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어(English) 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "    return [token.text.lower() for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAMTQyJizo9"
      },
      "source": [
        "* **필드(field)** 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
        "* 번역 목표\n",
        "    * 소스(SRC): 독일어\n",
        "    * 목표(TRG): 영어"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext"
      ],
      "metadata": {
        "id": "V-5boRShmNIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A94K670ni2P_"
      },
      "source": [
        "* 대표적인 영어-독어 번역 데이터셋인 **Multi30k**를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4QLSm3mmeM3",
        "outputId": "e493d798-909a-4fed-d9db-1870ba174096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchdata\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "train_dataset = Multi30k(root='.data', split=('train'), language_pair=(\"de\", \"en\"))"
      ],
      "metadata": {
        "id": "sx4XKAY5m2Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = []\n",
        "de = []\n",
        "for label, line in train_dataset:\n",
        "    de+=[tokenize_de(label)]\n",
        "    en+=[tokenize_en(line)]"
      ],
      "metadata": {
        "id": "a6nK_1TXm36i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyMoKXZenNSo",
        "outputId": "5ebd5bb7-3ae0-4c59-b7e0-e3d1580dab9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['two',\n",
              "  'young',\n",
              "  ',',\n",
              "  'white',\n",
              "  'males',\n",
              "  'are',\n",
              "  'outside',\n",
              "  'near',\n",
              "  'many',\n",
              "  'bushes',\n",
              "  '.'],\n",
              " ['several',\n",
              "  'men',\n",
              "  'in',\n",
              "  'hard',\n",
              "  'hats',\n",
              "  'are',\n",
              "  'operating',\n",
              "  'a',\n",
              "  'giant',\n",
              "  'pulley',\n",
              "  'system',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt6dbR43jMej"
      },
      "source": [
        "* **필드(field)** 객체의 **build_vocab** 메서드를 이용해 영어와 독어의 단어 사전을 생성합니다.\n",
        "  * **최소 2번 이상** 등장한 단어만을 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sos_token='<sos>'\n",
        "eos_token='<eos>'"
      ],
      "metadata": {
        "id": "ikV4MKYznVnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRG=torchtext.vocab.build_vocab_from_iterator(en,min_freq=2,specials=['<unk>','<pad>',sos_token,eos_token])\n",
        "SRC=torchtext.vocab.build_vocab_from_iterator(de,min_freq=2,specials=['<unk>','<pad>',sos_token,eos_token])\n",
        "TRG.set_default_index(0)\n",
        "SRC.set_default_index(0)"
      ],
      "metadata": {
        "id": "Sr1UUtXUnZ2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUs-Nf57jIMO",
        "outputId": "567e04aa-f3aa-478e-a4e4-79880021e5e5"
      },
      "source": [
        "print(len(TRG.vocab))\n",
        "print(len(SRC.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5893\n",
            "7853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC[sos_token]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_CGehSCnkAX",
        "outputId": "b21a6625-23b2-4b0d-bc0d-da6e42cad25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC[eos_token]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "las2cfMXnnLn",
        "outputId": "a7b6ea1d-8f81-4b9f-9af3-328cd9592ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC[\"adsafwe\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqllg8PQnneo",
        "outputId": "2f4a9f9f-356e-431a-f16c-5f746e8a365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC.vocab.get_stoi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGI7hMULnoBw",
        "outputId": "616af046-14ad-4c55-e870-0349269a1595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'‘': 7852,\n",
              " 'üppigen': 7851,\n",
              " 'überwacht': 7849,\n",
              " 'übersäte': 7848,\n",
              " 'überschlag': 7845,\n",
              " 'überreste': 7844,\n",
              " 'überlegt': 7843,\n",
              " 'überhang': 7841,\n",
              " 'überführung': 7839,\n",
              " 'überfluteten': 7838,\n",
              " 'überfliegt': 7837,\n",
              " 'öffentliches': 7833,\n",
              " 'äste': 7832,\n",
              " 'äpfel': 7830,\n",
              " 'ähnlichen': 7828,\n",
              " 'zügel': 7827,\n",
              " 'zwirbelt': 7825,\n",
              " 'zweispurigen': 7822,\n",
              " 'zweirad': 7821,\n",
              " 'zwanzig': 7819,\n",
              " 'zustand': 7818,\n",
              " 'zuschauerraum': 7817,\n",
              " 'zusammensein': 7816,\n",
              " 'zusammenpassenden': 7815,\n",
              " 'zusammengebaut': 7813,\n",
              " 'zuneigung': 7811,\n",
              " 'zuhörern': 7808,\n",
              " 'zuhält': 7807,\n",
              " 'zugfahrt': 7804,\n",
              " 'zugedeckt': 7802,\n",
              " 'zufährt': 7801,\n",
              " 'zufahrt': 7800,\n",
              " 'zitrone': 7797,\n",
              " 'zirkus': 7796,\n",
              " 'zielort': 7795,\n",
              " 'zielfernrohr': 7794,\n",
              " 'ziegelsteingebäude': 7793,\n",
              " 'zerzaust': 7791,\n",
              " 'zerrissenen': 7790,\n",
              " 'zerlumpter': 7789,\n",
              " 'zerlegen': 7788,\n",
              " 'zerkleinert': 7787,\n",
              " 'zerfetzten': 7786,\n",
              " 'zerbrochenen': 7784,\n",
              " 'zerbrochene': 7783,\n",
              " 'zerbricht': 7782,\n",
              " 'zentimeter': 7781,\n",
              " 'zementlaster': 7780,\n",
              " 'zeltplane': 7777,\n",
              " 'zeltet': 7776,\n",
              " 'zehn': 7775,\n",
              " 'zehenspitzen': 7774,\n",
              " 'zaubertrick': 7773,\n",
              " 'zauberer': 7772,\n",
              " 'zange': 7770,\n",
              " 'zahnstocher': 7769,\n",
              " 'zahnarzt': 7768,\n",
              " 'yankees': 7765,\n",
              " 'yamaha': 7764,\n",
              " 'wütend': 7763,\n",
              " 'würzt': 7762,\n",
              " 'würste': 7761,\n",
              " 'wünschen': 7759,\n",
              " 'wälzt': 7758,\n",
              " 'wälder': 7757,\n",
              " 'wrestler': 7754,\n",
              " 'worten': 7753,\n",
              " 'wolke': 7751,\n",
              " 'wohl': 7748,\n",
              " 'wodurch': 7747,\n",
              " 'wirkt': 7746,\n",
              " 'winterlichen': 7744,\n",
              " 'winterbekleidung': 7743,\n",
              " 'winkend': 7742,\n",
              " 'wildwasserrafting': 7741,\n",
              " 'wii': 7739,\n",
              " 'whirlpool': 7736,\n",
              " 'wetteifert': 7734,\n",
              " 'werferhügel': 7730,\n",
              " 'werdendem': 7729,\n",
              " 'wer': 7728,\n",
              " 'wenden': 7727,\n",
              " 'weißhaarige': 7724,\n",
              " 'weiß-braunem': 7721,\n",
              " 'weinflasche': 7718,\n",
              " 'weihnachtsmotiv': 7716,\n",
              " 'wettschwimmen': 7735,\n",
              " 'weiblich': 7712,\n",
              " 'wehrt': 7711,\n",
              " 'wegschaut': 7710,\n",
              " 'weges': 7709,\n",
              " 'wasserstrahl': 7708,\n",
              " 'wasserspritzer': 7707,\n",
              " 'wasserskulptur': 7704,\n",
              " 'wasserrohre': 7702,\n",
              " 'wasseroberfläche': 7700,\n",
              " 'wasserbrunnen': 7697,\n",
              " 'warteraum': 7694,\n",
              " 'wartende': 7693,\n",
              " 'wandgemälde': 7688,\n",
              " 'wandbemalung': 7687,\n",
              " 'waldgegend': 7684,\n",
              " 'wakeboarden': 7681,\n",
              " 'vorübergehend': 7678,\n",
              " 'vorzubereiten': 7676,\n",
              " 'vorstadthaus': 7674,\n",
              " 'vornübergebeugt': 7672,\n",
              " 'vornüber': 7671,\n",
              " 'vornehmen': 7670,\n",
              " 'vornehm': 7669,\n",
              " 'vorgarten': 7667,\n",
              " 'vorbeizukommen': 7664,\n",
              " 'vorbeikommt': 7661,\n",
              " 'volleyballplatz': 7658,\n",
              " 'volles': 7657,\n",
              " 'visier': 7654,\n",
              " 'vierte': 7652,\n",
              " 'vierrädrigen': 7651,\n",
              " 'vierergruppe': 7650,\n",
              " 'vielfältige': 7648,\n",
              " 'vieler': 7647,\n",
              " 'videokameras': 7646,\n",
              " 'videoaufnahme': 7645,\n",
              " 'vesammelt': 7644,\n",
              " 'verzweifelt': 7643,\n",
              " 'verziert': 7642,\n",
              " 'vertreiben': 7638,\n",
              " 'verteidigern': 7636,\n",
              " 'versorgt': 7635,\n",
              " 'verschwommenes': 7633,\n",
              " 'verschmiertem': 7630,\n",
              " 'verrückt': 7626,\n",
              " 'verregneten': 7625,\n",
              " 'vermischt': 7622,\n",
              " 'verlegen': 7620,\n",
              " 'verkäufers': 7619,\n",
              " 'verkäuferin': 7618,\n",
              " 'verkleidung': 7617,\n",
              " 'verkehrsschild': 7615,\n",
              " 'verkaufsautomaten': 7611,\n",
              " 'verhaftet': 7608,\n",
              " 'vergrößerungsglas': 7607,\n",
              " 'vergraben': 7606,\n",
              " 'verglasten': 7605,\n",
              " 'vereinzelten': 7604,\n",
              " 'veranstaltet': 7600,\n",
              " 'vader': 7597,\n",
              " 'urlaub': 7595,\n",
              " 'ureinwohnerin': 7593,\n",
              " 'unterstützen': 7591,\n",
              " 'unterirdischen': 7589,\n",
              " 'unterhosen': 7588,\n",
              " 'unterholz': 7587,\n",
              " 'unterhaltungskünstler': 7586,\n",
              " 'union': 7580,\n",
              " 'unglückliches': 7579,\n",
              " 'ungepflegten': 7577,\n",
              " 'unfallstelle': 7576,\n",
              " 'unfallort': 7575,\n",
              " 'unebenen': 7574,\n",
              " 'unbekannter': 7573,\n",
              " 'unbeholfen': 7571,\n",
              " 'umzugswagen': 7570,\n",
              " 'umzugs': 7569,\n",
              " 'umstehenden': 7568,\n",
              " 'umschlag': 7566,\n",
              " 'umgekippt': 7565,\n",
              " 'umgefallenen': 7564,\n",
              " 'umgebene': 7562,\n",
              " 'umdreht': 7559,\n",
              " 'umarmungen': 7558,\n",
              " 'ufernähe': 7557,\n",
              " 'u-bahn-zug': 7556,\n",
              " 'türkise': 7554,\n",
              " 'tüchern': 7552,\n",
              " 'töpfert': 7551,\n",
              " 'töpfe': 7550,\n",
              " 'töchter': 7549,\n",
              " 'tätowieren': 7547,\n",
              " 'tätigkeiten': 7546,\n",
              " 'turnübung': 7543,\n",
              " 'turnkleidung': 7542,\n",
              " 'turbanen': 7541,\n",
              " 'tuba': 7538,\n",
              " 'trösten': 7537,\n",
              " 'trägershirt': 7536,\n",
              " 'truthähne': 7533,\n",
              " 'zapft': 7771,\n",
              " 'trotzt': 7532,\n",
              " 'tropfen': 7530,\n",
              " 'trockene': 7529,\n",
              " 'trinkgeld': 7527,\n",
              " 'trauung': 7523,\n",
              " 'transport': 7522,\n",
              " 'transparenten': 7521,\n",
              " 'traditionell': 7516,\n",
              " 'toten': 7511,\n",
              " 'topfpflanzen': 7508,\n",
              " 'tollt': 7506,\n",
              " 'tintenfisch': 7502,\n",
              " 'trampelpfad': 7519,\n",
              " 'tim': 7501,\n",
              " 'theaterstück': 7494,\n",
              " 'text': 7492,\n",
              " 'tennisschuh': 7490,\n",
              " 'teilnehmende': 7486,\n",
              " 'teenager-jungen': 7483,\n",
              " 'technologie': 7481,\n",
              " 'teamkollege': 7479,\n",
              " 'tauschen': 7478,\n",
              " 'taubenschwarm': 7477,\n",
              " 'tatort': 7476,\n",
              " 'taste': 7474,\n",
              " 'tarnjacke': 7471,\n",
              " 'tarnfarben': 7469,\n",
              " 'tarnanzügen': 7468,\n",
              " 'tanzstudio': 7466,\n",
              " 'tamburin': 7464,\n",
              " 'takt': 7462,\n",
              " 'taille': 7461,\n",
              " 'süßigkeitenverkäufer': 7458,\n",
              " 'süßer': 7454,\n",
              " 'säuglings': 7452,\n",
              " 'szenerie': 7450,\n",
              " 'surfbrettern': 7448,\n",
              " 'suppe': 7447,\n",
              " 'sumpfiges': 7446,\n",
              " 'sumpfigen': 7445,\n",
              " 'sumpf': 7444,\n",
              " 'stürmischem': 7439,\n",
              " 'vase': 7598,\n",
              " 'stöckelschuhen': 7438,\n",
              " 'stängel': 7437,\n",
              " 'stände': 7436,\n",
              " 'sturm': 7432,\n",
              " 'stuhlreihen': 7430,\n",
              " 'sträuchern': 7427,\n",
              " 'struppiger': 7426,\n",
              " 'strumpfhosen': 7425,\n",
              " 'strommasten': 7424,\n",
              " 'stromleitungen': 7423,\n",
              " 'trittsteinen': 7528,\n",
              " 'striegelt': 7422,\n",
              " 'stretching': 7421,\n",
              " 'streitkräfte': 7420,\n",
              " 'streikschildern': 7418,\n",
              " 'streichorchester': 7417,\n",
              " 'straßenmusik': 7414,\n",
              " 'straßenlauf': 7412,\n",
              " 'straßenkreuzung': 7409,\n",
              " 'straßengeschäft': 7407,\n",
              " 'straßenansicht': 7406,\n",
              " 'strandspaziergang': 7405,\n",
              " 'strandkleid': 7404,\n",
              " 'strandbuggy': 7403,\n",
              " 'strahlendem': 7401,\n",
              " 'stoppschild': 7398,\n",
              " 'stofftieren': 7397,\n",
              " 'stoffen': 7396,\n",
              " 'verwitterten': 7640,\n",
              " 'stocks': 7395,\n",
              " 'stirnlampe': 7394,\n",
              " 'stilles': 7392,\n",
              " 'stiften': 7391,\n",
              " 'sticht': 7389,\n",
              " 'steuern': 7388,\n",
              " 'steuer': 7387,\n",
              " 'sternen': 7384,\n",
              " 'steppdecke': 7383,\n",
              " 'steinweg': 7382,\n",
              " 'steinsäulen': 7381,\n",
              " 'steinstruktur': 7380,\n",
              " 'steinoberfläche': 7378,\n",
              " 'steiniges': 7377,\n",
              " 'steinhaufen': 7376,\n",
              " 'steinbrücke': 7375,\n",
              " 'steinboden': 7374,\n",
              " 'statur': 7371,\n",
              " 'stattfindenden': 7369,\n",
              " 'startbereit': 7365,\n",
              " 'starker': 7364,\n",
              " 'stützrädern': 7442,\n",
              " 'stahlbalken': 7360,\n",
              " 'stahl': 7359,\n",
              " 'stadtteil': 7358,\n",
              " 'stadtplan': 7357,\n",
              " 'stadtbrunnen': 7353,\n",
              " 'stadtarbeiter': 7352,\n",
              " 'spät': 7349,\n",
              " 'sprüngen': 7347,\n",
              " 'sportteam': 7342,\n",
              " 'sportstudio': 7341,\n",
              " 'sportgerät': 7338,\n",
              " 'sportfeld': 7337,\n",
              " 'spirituosengeschäft': 7335,\n",
              " 'spinnkurs': 7333,\n",
              " 'verkaufstand': 7612,\n",
              " 'spießen': 7331,\n",
              " 'spielzeugtraktor': 7330,\n",
              " 'spielzeugs': 7329,\n",
              " 'spielzeugrutsche': 7328,\n",
              " 'spielzeuglastwagen': 7327,\n",
              " 'spielzeughaus': 7325,\n",
              " 'spielzeuggitarre': 7323,\n",
              " 'spielzeugeisenbahn': 7321,\n",
              " 'spieltunnel': 7318,\n",
              " 'spielkonsole': 7316,\n",
              " 'spielkarten': 7315,\n",
              " 'spielhaus': 7314,\n",
              " 'spiderman': 7308,\n",
              " 'speerwurf': 7304,\n",
              " 'sparring': 7302,\n",
              " 'spanischen': 7299,\n",
              " 'space': 7297,\n",
              " 'soße': 7296,\n",
              " 'sonniger': 7293,\n",
              " 'sonnenschirme': 7291,\n",
              " 'sonnenaufgang': 7288,\n",
              " 'sonnenanbetern': 7287,\n",
              " 'sonnebrille': 7286,\n",
              " 'song': 7285,\n",
              " 'sommerkleidung': 7284,\n",
              " 'sommerkleider': 7283,\n",
              " 'soldatinnen': 7282,\n",
              " 'softdrink': 7280,\n",
              " 'sockenhaltern': 7278,\n",
              " 'socke': 7277,\n",
              " 'skimboarden': 7274,\n",
              " 'skifahrerin': 7273,\n",
              " 'skibrille': 7272,\n",
              " 'skianzug': 7271,\n",
              " 'sketch': 7270,\n",
              " 'skateranlage': 7267,\n",
              " 'skateboardfahren': 7266,\n",
              " 'sitzung': 7262,\n",
              " 'sitze': 7261,\n",
              " 'situation': 7259,\n",
              " 'silbernem': 7256,\n",
              " 'signiert': 7254,\n",
              " 'sichtbare': 7252,\n",
              " 'sicherungsseilen': 7251,\n",
              " 'shampoo': 7248,\n",
              " 'set': 7247,\n",
              " 'sessellift': 7246,\n",
              " 'serviette': 7245,\n",
              " 'sense': 7243,\n",
              " 'sendet': 7241,\n",
              " 'seminar': 7240,\n",
              " 'seils': 7237,\n",
              " 'seifenwasser': 7235,\n",
              " 'sehe': 7234,\n",
              " 'verkehrsreiche': 7614,\n",
              " 'seahawks': 7230,\n",
              " 'schürt': 7228,\n",
              " 'schön': 7225,\n",
              " 'schärpe': 7224,\n",
              " 'schäferhunde': 7223,\n",
              " 'schäbig': 7222,\n",
              " 'schwitzender': 7220,\n",
              " 'schwingenden': 7219,\n",
              " 'schwimmkleidung': 7216,\n",
              " 'schwimmhilfen': 7215,\n",
              " 'schwimmhalle': 7214,\n",
              " 'vorhand': 7668,\n",
              " 'schwimmende': 7212,\n",
              " 'schwimmbekleidung': 7211,\n",
              " 'schwelle': 7209,\n",
              " 'schweinefleisch': 7207,\n",
              " 'schwarzweiß': 7206,\n",
              " 'schwarzhaariges': 7205,\n",
              " 'schwarz-orangefarbenen': 7202,\n",
              " 'trainingsanzügen': 7518,\n",
              " 'snap': 7275,\n",
              " 'schutzhelme': 7200,\n",
              " 'schuluniform': 7197,\n",
              " 'schulklasse': 7196,\n",
              " 'schulkindern': 7195,\n",
              " 'schulkantine': 7194,\n",
              " 'schulalter': 7191,\n",
              " 'schuhputzstand': 7190,\n",
              " 'schubst': 7189,\n",
              " 'schubkarren': 7188,\n",
              " 'schrägen': 7187,\n",
              " 'schroffen': 7186,\n",
              " 'schritten': 7185,\n",
              " 'schreienden': 7183,\n",
              " 'schraubenschlüssel': 7180,\n",
              " 'schraubendreher': 7179,\n",
              " 'schottischer': 7178,\n",
              " 'schottenröcken': 7175,\n",
              " 'schnürsenkel': 7173,\n",
              " 'schnüren': 7172,\n",
              " 'schnüffeln': 7171,\n",
              " 'schnurrbärtiger': 7170,\n",
              " 'schnurrbärten': 7169,\n",
              " 'verschiedenfarbiger': 7628,\n",
              " 'schneiderin': 7165,\n",
              " 'schneidbrett': 7164,\n",
              " 'schneesturm': 7163,\n",
              " 'stützen': 7441,\n",
              " 'schneeschuh': 7162,\n",
              " 'träg': 7534,\n",
              " 'schneeschaufel': 7161,\n",
              " 'schnees': 7160,\n",
              " 'schneejacke': 7158,\n",
              " 'schneebrille': 7157,\n",
              " 'schneeausrüstung': 7155,\n",
              " 'schnabel': 7153,\n",
              " 'stadtbus': 7354,\n",
              " 'schmiert': 7152,\n",
              " 'schluckt': 7149,\n",
              " 'schlittschuhe': 7148,\n",
              " 'schlittert': 7147,\n",
              " 'schlips': 7145,\n",
              " 'schlechten': 7141,\n",
              " 'schlechtem': 7140,\n",
              " 'schlangen': 7138,\n",
              " 'schlammiges': 7137,\n",
              " 'schlammigem': 7136,\n",
              " 'schlagmanns': 7135,\n",
              " 'schlafsäcken': 7134,\n",
              " 'schlafanzügen': 7133,\n",
              " 'schirme': 7131,\n",
              " 'schilden': 7128,\n",
              " 'schichten': 7124,\n",
              " 'scheune': 7123,\n",
              " 'scheucht': 7122,\n",
              " 'schaulustiger': 7118,\n",
              " 'schaulustigen': 7117,\n",
              " 'schaukelgestell': 7115,\n",
              " 'schaufensterbummel': 7114,\n",
              " 'saxophonisten': 7109,\n",
              " 'saxophone': 7108,\n",
              " 'saxofonspieler': 7107,\n",
              " 'sausen': 7106,\n",
              " 'sauberen': 7105,\n",
              " 'sanft': 7103,\n",
              " 'sandsturm': 7102,\n",
              " 'sandsack': 7101,\n",
              " 'sandkasten': 7100,\n",
              " 'saiteninstrumenten': 7097,\n",
              " 's': 7096,\n",
              " 'rückbank': 7094,\n",
              " 'rötlichem': 7093,\n",
              " 'röcke': 7092,\n",
              " 'räder': 7091,\n",
              " 'rund': 7089,\n",
              " 'rummelplatz': 7088,\n",
              " 'rot-grünen': 7084,\n",
              " 'rot-gelben': 7081,\n",
              " 'spielzeugfahrrad': 7322,\n",
              " 'rosa-grünen': 7079,\n",
              " 'rollerblader': 7075,\n",
              " 'rodgers': 7073,\n",
              " 'rodeoarena': 7072,\n",
              " 'rodeo-clown': 7071,\n",
              " 'rodeln': 7069,\n",
              " 'rockkonzert': 7066,\n",
              " 'riss': 7061,\n",
              " 'risiko': 7060,\n",
              " 'rinder': 7057,\n",
              " 'richtern': 7055,\n",
              " 'rettungskraft': 7053,\n",
              " 'rettungshelfer': 7052,\n",
              " 'restaurantmitarbeiter': 7050,\n",
              " 'reportern': 7048,\n",
              " 'rennmotorrad': 7045,\n",
              " 'rennhund': 7044,\n",
              " 'renaissance-fest': 7041,\n",
              " 'reißen': 7039,\n",
              " 'süßigkeit': 7456,\n",
              " 'reishut': 7037,\n",
              " 'reisfeld': 7036,\n",
              " 'regennassen': 7031,\n",
              " 'regenjacke': 7029,\n",
              " 'regenbogenfarbigen': 7028,\n",
              " 'reckt': 7026,\n",
              " 'rechteckigen': 7025,\n",
              " 'reaktion': 7024,\n",
              " 'rauen': 7021,\n",
              " 'rauchwolke': 7020,\n",
              " 'rasterlocken': 7019,\n",
              " 'rasten': 7018,\n",
              " 'rasseln': 7017,\n",
              " 'rasiertem': 7016,\n",
              " 'querflöte': 7009,\n",
              " 'päckchen': 7006,\n",
              " 'pusten': 7005,\n",
              " 'pulver': 7002,\n",
              " 'pulk': 7000,\n",
              " 'prächtigen': 6997,\n",
              " 'prächtige': 6996,\n",
              " 'skateboard-kunststück': 7263,\n",
              " 'prächtig': 6995,\n",
              " 'protestierende': 6994,\n",
              " 'professionell': 6992,\n",
              " 'probe': 6990,\n",
              " 'pro': 6989,\n",
              " 'pritschenwagen': 6988,\n",
              " 'presslufthammer': 6986,\n",
              " 'prallen': 6982,\n",
              " 'powerpoint-präsentation': 6980,\n",
              " 'postmitarbeiter': 6978,\n",
              " 'porträts': 6976,\n",
              " 'polo-hemd': 6973,\n",
              " 'polizeifahrzeug': 6971,\n",
              " 'polizeiauto': 6969,\n",
              " 'plüschtieren': 6968,\n",
              " 'platzt': 6966,\n",
              " 'plattenladen': 6965,\n",
              " 'plastikeimer': 6961,\n",
              " 'plakate': 6959,\n",
              " 'pitbull': 6958,\n",
              " 'schreibend': 7182,\n",
              " 'pink-weißem': 6957,\n",
              " 'pinien': 6956,\n",
              " 'pinguine': 6955,\n",
              " 'pilz': 6954,\n",
              " 'stämmiger': 7435,\n",
              " 'pickup-truck': 6952,\n",
              " 'piano': 6951,\n",
              " 'pfote': 6949,\n",
              " 'pferdewagen': 6946,\n",
              " 'pferdeschwänzen': 6945,\n",
              " 'pferdefuhrwerk': 6944,\n",
              " 'pferch': 6943,\n",
              " 'pfauenkostüm': 6941,\n",
              " 'pfau': 6940,\n",
              " 'pfarrer': 6939,\n",
              " 'rolltreppen': 7077,\n",
              " 'perücken': 6936,\n",
              " 'personenzug': 6935,\n",
              " 'penn': 6932,\n",
              " 'pelikan': 6930,\n",
              " 'pedale': 6929,\n",
              " 'pavillon': 6927,\n",
              " 'patrouilliert': 6926,\n",
              " 'patriotischen': 6925,\n",
              " 'pastellfarbenen': 6923,\n",
              " 'passieren': 6922,\n",
              " 'passender': 6921,\n",
              " 'passendem': 6920,\n",
              " 'u-bahn-bahnsteig': 7555,\n",
              " 'passagier': 6918,\n",
              " 'parkuhr': 6914,\n",
              " 'parka': 6912,\n",
              " 'pappschachtel': 6909,\n",
              " 'papierteller': 6908,\n",
              " 'papagei': 6906,\n",
              " 'palmwedeln': 6905,\n",
              " 'pakete': 6904,\n",
              " 'paintball': 6903,\n",
              " 'pain': 6902,\n",
              " 'pabst': 6901,\n",
              " 'outdoor-bekleidung': 6900,\n",
              " 'out': 6899,\n",
              " 'orten': 6898,\n",
              " 'orioles': 6897,\n",
              " 'orgel': 6896,\n",
              " 'organgefarbenen': 6894,\n",
              " 'orange-grauen': 6890,\n",
              " 'orange-blauen': 6889,\n",
              " 'opfer': 6888,\n",
              " 'open-air-konzert': 6886,\n",
              " 'olympische': 6884,\n",
              " 'ohrringen': 6881,\n",
              " 'ohrhörer': 6880,\n",
              " 'objektiv': 6878,\n",
              " 'oberleitungsfahrzeug': 6877,\n",
              " 'obama': 6876,\n",
              " 'oakland': 6875,\n",
              " 'nächtliches': 6873,\n",
              " 'notre': 6869,\n",
              " 'notiz': 6868,\n",
              " 'notenblättern': 6867,\n",
              " 'notenblatt': 6866,\n",
              " 'normaler': 6862,\n",
              " 'no': 6860,\n",
              " 'nische': 6859,\n",
              " 'niemand': 6858,\n",
              " 'niedrig': 6857,\n",
              " 'new-york-hut': 6856,\n",
              " 'neugieriges': 6855,\n",
              " 'neugeborenen': 6854,\n",
              " 'neugeborene': 6853,\n",
              " 'neongrün': 6851,\n",
              " 'neongelben': 6850,\n",
              " 'neonfarbenen': 6849,\n",
              " 'namensschildern': 6844,\n",
              " 'tribünen': 7526,\n",
              " 'nahrung': 6842,\n",
              " 'nadelstreifenhemd': 6840,\n",
              " 'nacken': 6839,\n",
              " 'nachrichtensprecher': 6838,\n",
              " 'nachricht': 6837,\n",
              " 'nachgehen': 6835,\n",
              " 'nachdenklich': 6834,\n",
              " 'nachbarn': 6832,\n",
              " 'mürrisch': 6831,\n",
              " 'münztelefon': 6830,\n",
              " 'münzfernsprecher': 6829,\n",
              " 'mülltonnen': 6828,\n",
              " 'möglich': 6826,\n",
              " 'möbelgeschäft': 6825,\n",
              " 'männerteam': 6824,\n",
              " 'mädchenmannschaften': 6822,\n",
              " 'mädchengruppe': 6821,\n",
              " 'mustang': 6820,\n",
              " 'muslimischer': 6819,\n",
              " 'typischen': 7545,\n",
              " 'muslimische': 6818,\n",
              " 'musikerin': 6815,\n",
              " 'muffins': 6812,\n",
              " 'mountainbike-fahrer': 6811,\n",
              " 'motorschlitten': 6810,\n",
              " 'motorrads': 6809,\n",
              " 'motorradhelm': 6807,\n",
              " 'motorradfahrerin': 6806,\n",
              " 'motorradfahren': 6805,\n",
              " 'molliges': 6799,\n",
              " 'modischer': 6796,\n",
              " 'moderner': 6793,\n",
              " 'modelliert': 6792,\n",
              " 'mittelpunkt': 6788,\n",
              " 'mittelgroße': 6787,\n",
              " 'mitgliedern': 6786,\n",
              " 'mitfahrgelegenheit': 6785,\n",
              " 'mischen': 6784,\n",
              " 'minnesota': 6782,\n",
              " 'minirock': 6781,\n",
              " 'militärischen': 6780,\n",
              " 'militärhosen': 6779,\n",
              " 'mikroskopen': 6778,\n",
              " 'mike': 6777,\n",
              " 'michigan': 6775,\n",
              " 'michael': 6774,\n",
              " 'mexiko': 6773,\n",
              " 'metzgerei': 6772,\n",
              " 'metallstruktur': 6768,\n",
              " 'metallrahmen': 6765,\n",
              " 'metallgerüst': 6762,\n",
              " 'metalldach': 6759,\n",
              " 'metallbauwerk': 6758,\n",
              " 'metallarbeiter': 6756,\n",
              " 'menschliche': 6754,\n",
              " 'menschenleeren': 6752,\n",
              " 'meißel': 6751,\n",
              " 'mehrfarbiger': 6749,\n",
              " 'meereswellen': 6747,\n",
              " 'meeresfrüchte': 6745,\n",
              " 'matrosenanzug': 6742,\n",
              " 'matrosen': 6741,\n",
              " 'maskierter': 6736,\n",
              " 'marmor': 6734,\n",
              " 'marker': 6732,\n",
              " 'marineblauer': 6731,\n",
              " 'marineblau': 6730,\n",
              " 'marathons': 6729,\n",
              " 'marathonläufern': 6728,\n",
              " 'manns': 6727,\n",
              " 'mandel': 6725,\n",
              " 'transformator': 7520,\n",
              " 'mammutbaum': 6724,\n",
              " 'malerischen': 6723,\n",
              " 'majestätischen': 6721,\n",
              " 'mahl': 6719,\n",
              " 'sumoringer': 7443,\n",
              " 'magazin': 6718,\n",
              " 'mach': 6717,\n",
              " 'mac-computer': 6716,\n",
              " 'm': 6715,\n",
              " 'löwenzahnblüte': 6713,\n",
              " 'löffeln': 6712,\n",
              " 'lätzchen': 6711,\n",
              " 'ländliche': 6707,\n",
              " 'lutscht': 6706,\n",
              " 'lustiges': 6704,\n",
              " 'lungern': 6702,\n",
              " 'luftmatratzen': 6701,\n",
              " 'luftballontiere': 6700,\n",
              " 'louvre': 6699,\n",
              " 'londoner': 6698,\n",
              " 'lolli': 6697,\n",
              " 'lokomotiven': 6696,\n",
              " 'lokomotive': 6695,\n",
              " 'livekonzert': 6693,\n",
              " 'linsen': 6692,\n",
              " 'linkshändiger': 6690,\n",
              " 'linkes': 6689,\n",
              " 'notebooks': 6865,\n",
              " 'lineal': 6688,\n",
              " 'limonaden': 6687,\n",
              " 'lilane': 6685,\n",
              " 'schulen': 7192,\n",
              " 'pedal': 6928,\n",
              " 'lilafarbenem': 6684,\n",
              " 'lieben': 6679,\n",
              " 'lichtung': 6678,\n",
              " 'lichts': 6677,\n",
              " 'leuchtturm': 6676,\n",
              " 'leuchtreklame': 6675,\n",
              " 'leuchtender': 6674,\n",
              " 'leuchtendem': 6673,\n",
              " 'lesenden': 6672,\n",
              " 'leitung': 6669,\n",
              " 'leitplanken': 6668,\n",
              " 'leht': 6667,\n",
              " 'lehnstuhl': 6666,\n",
              " 'leerstehenden': 6665,\n",
              " 'lederoberteil': 6660,\n",
              " 'lebron': 6657,\n",
              " 'lebensmittelbereich': 6654,\n",
              " 'spachtel': 7298,\n",
              " 'lebens': 6653,\n",
              " 'lebendigen': 6652,\n",
              " 'leadsänger': 6651,\n",
              " 'lautsprechern': 6649,\n",
              " 'laut': 6648,\n",
              " 'laufstrecke': 6646,\n",
              " 'laufschuhen': 6645,\n",
              " 'laufoberteil': 6644,\n",
              " 'laufhosen': 6642,\n",
              " 'laufgitter': 6641,\n",
              " 'laufenden': 6640,\n",
              " 'laubwerk': 6639,\n",
              " 'laternenpfahl': 6634,\n",
              " 'lateinamerikaner': 6632,\n",
              " 'las': 6631,\n",
              " 'langgeht': 6626,\n",
              " 'lang': 6625,\n",
              " 'landwirtschaftlichen': 6624,\n",
              " 'stürmt': 7440,\n",
              " 'landwirt': 6623,\n",
              " 'lakers': 6620,\n",
              " 'tiers': 7500,\n",
              " 'laken': 6619,\n",
              " 'lagerraum': 6618,\n",
              " 'lagerhalle': 6617,\n",
              " 'ladengeschäft': 6616,\n",
              " 'lachenden': 6611,\n",
              " 'labradoodle': 6610,\n",
              " 'künstlerischen': 6603,\n",
              " 'künstlerische': 6602,\n",
              " 'kühlboxen': 6600,\n",
              " 'kühlbox': 6599,\n",
              " 'kühe': 6598,\n",
              " 'küchenutensilien': 6597,\n",
              " 'küchenmaschine': 6596,\n",
              " 'kurzes': 6593,\n",
              " 'kurven': 6592,\n",
              " 'kunstvoll': 6591,\n",
              " 'kunststofftasse': 6590,\n",
              " 'kunststofffolie': 6589,\n",
              " 'kunstprojekt': 6587,\n",
              " 'kulturellen': 6583,\n",
              " 'kultur': 6582,\n",
              " 'kugelbad': 6581,\n",
              " 'krügen': 6580,\n",
              " 'kreuzen': 6576,\n",
              " 'kreativ': 6574,\n",
              " 'krankenpfleger': 6573,\n",
              " 'krankenhauskleidung': 6572,\n",
              " 'kostümierter': 6571,\n",
              " 'kostenlose': 6570,\n",
              " 'kosten': 6569,\n",
              " 'korpulentes': 6568,\n",
              " 'korken': 6567,\n",
              " 'konserven': 6560,\n",
              " 'nuckelt': 6870,\n",
              " 'kongress': 6559,\n",
              " 'kommunikationsgerät': 6558,\n",
              " 'komisches': 6557,\n",
              " 'komischer': 6556,\n",
              " 'kochmütze': 6551,\n",
              " 'knöpfe': 6549,\n",
              " 'knietief': 6546,\n",
              " 'kniestrümpfe': 6545,\n",
              " 'kniender': 6544,\n",
              " 'knapp': 6542,\n",
              " 'klub': 6540,\n",
              " 'klippen': 6539,\n",
              " 'kleinkinds': 6536,\n",
              " 'kleinbusses': 6535,\n",
              " 'kleidungsstück': 6534,\n",
              " 'konzertaufführung': 6561,\n",
              " 'klassischen': 6533,\n",
              " 'kitzelt': 6531,\n",
              " 'kirmesattraktion': 6526,\n",
              " 'kirchenchor': 6525,\n",
              " 'kippt': 6524,\n",
              " 'kinderschaukel': 6520,\n",
              " 'kinderbuch': 6519,\n",
              " 'kimono': 6517,\n",
              " 'kiesstrand': 6516,\n",
              " 'kieshügel': 6515,\n",
              " 'kieselsteine': 6514,\n",
              " 'khaki-hosen': 6513,\n",
              " 'kettenkarussell': 6512,\n",
              " 'kennzeichnet': 6510,\n",
              " 'keks': 6509,\n",
              " 'kegelförmigen': 6507,\n",
              " 'kawasaki': 6506,\n",
              " 'waschmaschinen': 7696,\n",
              " 'kauen': 6505,\n",
              " 'katzenbaby': 6504,\n",
              " 'katzenbabies': 6503,\n",
              " 'kastanienbraunes': 6502,\n",
              " 'kasino': 6501,\n",
              " 'kartoffelchip': 6500,\n",
              " 'karge': 6499,\n",
              " 'karatekampf': 6497,\n",
              " 'karate-klasse': 6496,\n",
              " 'kaputter': 6495,\n",
              " 'kampfsportturnier': 6491,\n",
              " 'kampfanzügen': 6490,\n",
              " 'kameracrew': 6488,\n",
              " 'kaltem': 6487,\n",
              " 'kais': 6485,\n",
              " 'life': 6681,\n",
              " 'kahlrasiertem': 6484,\n",
              " 'kaffeehaus': 6482,\n",
              " 'jüdischer': 6480,\n",
              " 'jüdischen': 6479,\n",
              " 'jüdische': 6478,\n",
              " 'juweliergeschäft': 6477,\n",
              " 'jump': 6474,\n",
              " 'joe': 6472,\n",
              " 'jim': 6471,\n",
              " 'jerusalem': 6470,\n",
              " 'jede': 6467,\n",
              " 'jeans-shorts': 6465,\n",
              " 'japanisches': 6464,\n",
              " 'jahr': 6459,\n",
              " 'italien': 6458,\n",
              " 'irokesenhaarschnitt': 6457,\n",
              " 'irgendwohin': 6456,\n",
              " 'irgendwelche': 6455,\n",
              " 'irgendeines': 6454,\n",
              " 'polizeibeamter': 6970,\n",
              " 'ipad': 6452,\n",
              " 'iowa': 6451,\n",
              " 'interessant': 6449,\n",
              " 'insekten': 6448,\n",
              " 'innenstadtbereich': 6445,\n",
              " 'innenräumen': 6444,\n",
              " 'soda': 7279,\n",
              " 'inline-skater': 6442,\n",
              " 'informellen': 6441,\n",
              " 'informell': 6440,\n",
              " 'inderinnen': 6438,\n",
              " 'immer': 6437,\n",
              " 'schwimmwettkampf': 7218,\n",
              " 'imbisswagen': 6436,\n",
              " 'imbiss': 6434,\n",
              " 'iglu': 6433,\n",
              " 'säcke': 7451,\n",
              " 'ice': 6432,\n",
              " 'hütten': 6431,\n",
              " 'hütet': 6430,\n",
              " 'hütchen': 6429,\n",
              " 'hündin': 6428,\n",
              " 'verbirgt': 7602,\n",
              " 'hühner': 6427,\n",
              " 'fahrradreifen': 3204,\n",
              " 'und': 10,\n",
              " 'touristin': 7513,\n",
              " 'elternteil': 3193,\n",
              " 'eishockeyspiel': 3190,\n",
              " 'durchgang': 3184,\n",
              " 'dicker': 3179,\n",
              " 'derby': 3176,\n",
              " 'unterhaltung': 4177,\n",
              " 'deckt': 3175,\n",
              " 'collie': 3173,\n",
              " 'hinauf': 396,\n",
              " 'dieselbe': 2279,\n",
              " 'gericht': 2314,\n",
              " 'brät': 3164,\n",
              " 'buchladen': 5747,\n",
              " 'brechen': 3162,\n",
              " 'parkgarage': 6913,\n",
              " 'blätterhaufen': 3159,\n",
              " 'gemauerten': 6183,\n",
              " 'blond': 3155,\n",
              " 'erdboden': 3196,\n",
              " 'männe': 6823,\n",
              " 'bewundern': 3151,\n",
              " 'geschminkt': 2848,\n",
              " 'gürtel': 1821,\n",
              " 'bekleidungsgeschäft': 3143,\n",
              " 'diesem': 930,\n",
              " 'turnübungen': 7544,\n",
              " 'begleiter': 3142,\n",
              " 'wasserfall': 1053,\n",
              " 'kleinkind': 299,\n",
              " 'bass': 3134,\n",
              " 'bambus': 3130,\n",
              " 'stadtpark': 2411,\n",
              " 'babyschaukel': 3123,\n",
              " 'feldhockey': 3206,\n",
              " 'wobei': 601,\n",
              " 'kabinen': 6481,\n",
              " 'außerdem': 3122,\n",
              " 'erwachsene': 330,\n",
              " '6': 3102,\n",
              " 'jongleur': 6473,\n",
              " 'schuhputzer': 5059,\n",
              " 'ausflug': 3118,\n",
              " 'waldlandschaft': 7685,\n",
              " 'erzählt': 3202,\n",
              " 'gymnastik': 3255,\n",
              " 'ans': 3108,\n",
              " 'rücksitz': 2008,\n",
              " 'entlanggehen': 3702,\n",
              " 'gespielt': 2316,\n",
              " 'zeitungen': 3090,\n",
              " 'ansammlung': 3109,\n",
              " 'zebrastreifen': 3089,\n",
              " 'süßigkeitenladen': 7457,\n",
              " 'drauf': 5861,\n",
              " 'festival': 3213,\n",
              " 'wände': 3087,\n",
              " 'weißhaariger': 3079,\n",
              " 'weiß-schwarzer': 3078,\n",
              " 'wandmalerei': 3072,\n",
              " 'anzeige': 4288,\n",
              " 'vorbeifahren': 3069,\n",
              " 'spielzeughammer': 7324,\n",
              " 'verärgert': 3064,\n",
              " 'schirmt': 7132,\n",
              " 'verfolgen': 3059,\n",
              " 'umgestürzten': 3054,\n",
              " 'handtasche': 912,\n",
              " 'windjacke': 4217,\n",
              " 'türkisfarbenen': 3052,\n",
              " 'tische': 3046,\n",
              " 'gerichtet': 2847,\n",
              " 'tennisschuhen': 3042,\n",
              " 'badebekleidung': 3604,\n",
              " 'betten': 5659,\n",
              " 'teilnimmt': 3040,\n",
              " 'straßenszene': 2417,\n",
              " 'teenagerin': 3039,\n",
              " 'lötet': 3913,\n",
              " 'stöbern': 3035,\n",
              " 'japanischen': 1831,\n",
              " 'turnschuhe': 5216,\n",
              " 'studiert': 3034,\n",
              " 'musikgruppe': 2161,\n",
              " 'stützt': 3473,\n",
              " 'einhändigen': 5903,\n",
              " 'seitenlinie': 3015,\n",
              " 'seilspringen': 3014,\n",
              " 'schwimmflügeln': 3010,\n",
              " 'rennautos': 4986,\n",
              " 'schwarz-braunen': 3006,\n",
              " 'schwarz-braune': 3005,\n",
              " 'schmutziges': 2999,\n",
              " 'kopfhörern': 1187,\n",
              " 'schachbrett': 7110,\n",
              " 'schmetterling': 2997,\n",
              " 'schlittschuh': 2996,\n",
              " 'stolpert': 3467,\n",
              " 'schlafzimmer': 2993,\n",
              " 'touristengruppe': 5201,\n",
              " 'saiteninstrument': 2988,\n",
              " 'sein': 132,\n",
              " 'ruhigem': 2986,\n",
              " 'fressen': 6097,\n",
              " 'ruft': 2985,\n",
              " 'fahrradrahmen': 5987,\n",
              " 'rot-weißes': 2983,\n",
              " 'rot-schwarzer': 2981,\n",
              " 'nassgespritzt': 6845,\n",
              " 'einkäufer': 2524,\n",
              " 'rennfahrer': 2979,\n",
              " 'raufen': 2978,\n",
              " 'rasenmäher': 2976,\n",
              " 'genüsslich': 3770,\n",
              " 'quad': 2974,\n",
              " 'drängen': 3180,\n",
              " 'backsteinwand': 3124,\n",
              " 'pyramide': 2973,\n",
              " 'puppen': 2972,\n",
              " 'gasmaske': 4574,\n",
              " 'punkten': 2971,\n",
              " 'produkt': 2968,\n",
              " 'popcorn': 2965,\n",
              " 'stroh': 3471,\n",
              " 'rentier': 7046,\n",
              " 'luftballons': 1242,\n",
              " 'pflückt': 2960,\n",
              " 'schnurrbart': 1197,\n",
              " 'frankreich': 4552,\n",
              " 'schlittenhunde': 7146,\n",
              " 'holzstamm': 6402,\n",
              " 'brotstand': 5739,\n",
              " 'ochsen': 2954,\n",
              " 'samen': 7098,\n",
              " 'industriegebäude': 4727,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKuySa3njQpB"
      },
      "source": [
        "* 한 문장에 포함된 단어가 연속적으로 **LSTM**에 입력되어야 합니다.\n",
        "    * 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋습니다.\n",
        "    * 이를 위해 BucketIterator를 사용합니다.\n",
        "    * **배치 크기(batch size)**: 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXKMVjtjPDm"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    de_list,en_list=[],[]\n",
        "    for (_de,_en) in batch:\n",
        "        de_list.append(torch.tensor(SRC([sos_token]+tokenize_de(_de.lower())+[eos_token]),dtype=torch.int64))\n",
        "        en_list.append(torch.tensor(TRG([sos_token]+tokenize_en(_en.lower())+[eos_token]),dtype=torch.int64))\n",
        "    de_inputs = torch.nn.utils.rnn.pad_sequence(de_list, batch_first=True,padding_value=1)\n",
        "    en_inputs = torch.nn.utils.rnn.pad_sequence(en_list, batch_first=True,padding_value=1)\n",
        "    return (de_inputs.contiguous(),en_inputs.contiguous())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5_krqaxjSPe"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator,test_iterator,valid_iterator = iter(Multi30k(root='.data', split=('train','test','valid'), language_pair=(\"de\", \"en\")))\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iterator, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_iterator, batch_size=BATCH_SIZE,collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJuTHG0gjXuU"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 주어진 소스 문장을 **문맥 벡터(context vector)로 인코딩**합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "    * **embed_dim**: 임베딩(embedding) 차원\n",
        "    * **enc_hidden_dim**: 인코더의 히든 상태(hidden state) 차원\n",
        "    * **dec_hidden_dim**: 디코더의 히든 상태(hidden state) 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnjffnbYjTQu"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 인코더(Encoder) 아키텍처 정의\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n",
        "    def forward(self, src):\n",
        "        # src: [단어 개수, 배치 크기]: 각 단어의 인덱스(index) 정보\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [단어 개수, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 문맥 벡터(context vector) 반환\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypx2kLydpu7D"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "- 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
        "- LSTM은 hidden state과 cell state을 반환합니다.\n",
        "- 하이퍼 파라미터(hyperparameter)\n",
        "  - input_dim: 하나의 단어에 대한 원핫 인코딩 차원\n",
        "  - embed_dim: 임베딩(embedding) 차원\n",
        "  - hidden_dim: 히든 상태(hidden state) 차원\n",
        "  - n_layers: RNN 레이어의 개수\n",
        "  - dropout_ratio: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d50749vHphJ3"
      },
      "source": [
        "# 디코더(Decoder) 아키텍처 정의\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n",
        "        \n",
        "        # FC 레이어 (인코더와 구조적으로 다른 부분)\n",
        "        self.output_dim = output_dim\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        # 드롭아웃(dropout)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        # cell = context: [레이어 개수, 배치 크기, 히든 차원]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input: [단어 개수 = 1, 배치 크기]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output: [단어 개수 = 1, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n",
        "        # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "        # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
        "\n",
        "        # 단어 개수는 어차피 1개이므로 차원 제거\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [배치 크기, 출력 차원]\n",
        "        \n",
        "        # (현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaTa3VRuOHl"
      },
      "source": [
        "#### **Seq2Seq 아키텍처**\n",
        "- 앞서 정의한 인코더(encoder)와 디코더(decoder)를 가지고 있는 하나의 아키텍처입니다.\n",
        "  - 인코더(encoder): 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩합니다.\n",
        "  - 디코더(decoder): 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
        "  - 단, 디코더는 한 단어씩 넣어서 한 번씩 결과를 구합니다.\n",
        "\n",
        "- Teacher forcing: 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiATVJjbuNC2"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [단어 개수, 배치 크기]\n",
        "        # trg: [단어 개수, 배치 크기]\n",
        "        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
        "        trg_len = trg.shape[0] # 단어 개수\n",
        "        batch_size = trg.shape[1] # 배치 크기\n",
        "        trg_vocab_size = self.decoder.output_dim # 출력 차원\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 첫 번째 입력은 항상 <sos> 토큰\n",
        "        input = trg[0, :]\n",
        "        \n",
        "        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
        "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
        "            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n",
        "  \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJG_szt-2MSd"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCYhFAB83GLx"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENCODER_EMBED_DIM = 256\n",
        "DECODER_EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT_RATIO = 0.5\n",
        "DEC_DROPOUT_RATIO = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K22n1KKM3ww8"
      },
      "source": [
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n",
        "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n",
        "\n",
        "# Seq2Seq 객체 선언\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QdXa2v2lgx"
      },
      "source": [
        "* **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-a6QGT3K1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c85279-47f7-43ff-b1f8-78273732f3ec"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7853, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW87m5Me39Al"
      },
      "source": [
        "* 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIE2upE3tkJ"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "TRG_PAD_IDX = TRG[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofZzAhn3_xn"
      },
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "    count=0\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for src,trg in iterator:\n",
        "        src = torch.transpose(src,0,1).to(device)\n",
        "        trg = torch.transpose(trg,0,1).to(device)\n",
        "        count+=1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "        \n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    \n",
        "    return epoch_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAt0uNnm4BVK"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "    count=0\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for src,trg in iterator:\n",
        "            src = torch.transpose(src,0,1).to(device)\n",
        "            trg = torch.transpose(trg,0,1).to(device)\n",
        "            count+=1\n",
        "            # 평가할 때 teacher forcing는 사용하지 않음\n",
        "            output = model(src, trg, 0)\n",
        "            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # 출력 단어의 인덱스 0은 사용하지 않음\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niB7_VVZ4Ptc"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Z-IBW84FiK"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22OJL3xx4RJ5"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxMwo05X8JIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "79f2f55e-6be2-41f4-a85e-9a1e19ac4c76"
      },
      "source": [
        "# 학습된 모델 저장\n",
        "from google.colab import files\n",
        "\n",
        "files.download('seq2seq_with_attention.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b8834770-92df-4df4-97e5-291d17a93cf4\", \"seq2seq_with_sequential.pt\", 82081062)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAAwxrgE9ujD"
      },
      "source": [
        "#### **모델 최종 테스트(testing) 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CHFydtw44xZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b04205-5e67-4842-aeea-dc409ed71eae"
      },
      "source": [
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EQkAXuEiglpMgofwF0HAp5oBhFspDt7_V3JBtEiCAKNbjw?download=1 -O seq2seq_with_attention.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 08:24:24--  https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EQkAXuEiglpMgofwF0HAp5oBhFspDt7_V3JBtEiCAKNbjw?download=1\n",
            "Resolving postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)... 13.107.136.9\n",
            "Connecting to postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/dongbinna_postech_ac_kr/Documents/Research/models/seq2seq_with_attention.pt?originalPath=aHR0cHM6Ly9wb3N0ZWNoYWNrci1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9kb25nYmlubmFfcG9zdGVjaF9hY19rci9FUWtBWHVFaWdscE1nb2Z3RjBIQXA1b0JoRnNwRHQ3X1YzSkJ0RWlDQUtOYmp3P3J0aW1lPVNwdXZnOUtWMkVn [following]\n",
            "--2020-12-01 08:24:25--  https://postechackr-my.sharepoint.com/personal/dongbinna_postech_ac_kr/Documents/Research/models/seq2seq_with_attention.pt?originalPath=aHR0cHM6Ly9wb3N0ZWNoYWNrci1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9kb25nYmlubmFfcG9zdGVjaF9hY19rci9FUWtBWHVFaWdscE1nb2Z3RjBIQXA1b0JoRnNwRHQ3X1YzSkJ0RWlDQUtOYmp3P3J0aW1lPVNwdXZnOUtWMkVn\n",
            "Reusing existing connection to postechackr-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82081062 (78M) [application/octet-stream]\n",
            "Saving to: ‘seq2seq_with_attention.pt’\n",
            "\n",
            "seq2seq_with_attent 100%[===================>]  78.28M  19.1MB/s    in 4.1s    \n",
            "\n",
            "2020-12-01 08:24:30 (19.1 MB/s) - ‘seq2seq_with_attention.pt’ saved [82081062/82081062]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrMv5cD29yKI",
        "outputId": "c1c9e981-f6fa-43ff-f559-ee203b32a41e"
      },
      "source": [
        "model.load_state_dict(torch.load('seq2seq_with_attention.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 3.181 | Test PPL: 24.074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSjFAq_091iF"
      },
      "source": [
        "#### **나만의 데이터로 모델 사용해보기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZuRXGB592W9"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        enc_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden, enc_outputs)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5MpIAAo93UE",
        "outputId": "dd99d305-772f-4886-fef6-4f4f1dc3c35f"
      },
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_dataset.examples[example_idx])['src']\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "소스 문장: ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n",
            "타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n",
            "전체 소스 토큰: ['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '<eos>']\n",
            "소스 문장 인덱스: [2, 8, 364, 10, 134, 70, 624, 565, 19, 780, 200, 20, 88, 4, 3]\n",
            "모델 출력 결과: a mother and her son enjoying a beautiful day . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpZnxDn894jW",
        "outputId": "b55c3cc0-4a63-45ba-c3e5-f5ac33cded14"
      },
      "source": [
        "src = tokenize_de(\"Guten Abend.\")\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "소스 문장: ['Guten', 'Abend', '.']\n",
            "전체 소스 토큰: ['<sos>', 'guten', 'abend', '.', '<eos>']\n",
            "소스 문장 인덱스: [2, 3799, 1163, 4, 3]\n",
            "모델 출력 결과: local villagers are preparing to dusk . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}